
@article{lewicki_efficient_2002,
	title = {Efficient coding of natural sounds},
	volume = {5},
	copyright = {2002 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn831},
	doi = {10.1038/nn831},
	abstract = {The auditory system encodes sound by decomposing the amplitude signal arriving at the ear into multiple frequency bands whose center frequencies and bandwidths are approximately exponential functions of the distance from the stapes. This organization is thought to result from the adaptation of cochlear mechanisms to the animal's auditory environment. Here we report that several basic auditory nerve fiber tuning properties can be accounted for by adapting a population of filter shapes to encode natural sounds efficiently. The form of the code depends on sound class, resembling a Fourier transformation when optimized for animal vocalizations and a wavelet transformation when optimized for non-biological environmental sounds. Only for the combined set does the optimal code follow scaling characteristics of physiological data. These results suggest that auditory nerve fibers encode a broad set of natural sounds in a manner consistent with information theoretic principles.},
	language = {en},
	number = {4},
	urldate = {2019-03-27TZ},
	journal = {Nature Neuroscience},
	author = {Lewicki, Michael S.},
	month = apr,
	year = {2002},
	pages = {356--363}
}

@article{tzanetakis_musical_2002,
	title = {Musical genre classification of audio signals},
	volume = {10},
	issn = {1063-6676},
	doi = {10.1109/TSA.2002.800560},
	abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61\% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
	number = {5},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Tzanetakis, G. and Cook, P.},
	month = jul,
	year = {2002},
	keywords = {Computer science, Cultural differences, Feature extraction, Humans, Instruments, Multiple signal classification, Music information retrieval, Pattern recognition, Signal analysis, Wavelet analysis, World Wide Web, audio signal processing, audio signals, automatic musical genre classification, content-based analysis, feature extraction, feature sets, genre hierarchies, harmonic content, human musical genre classification, information retrieval, instrumentation, music, music information retrieval systems, musical genre annotation, musical signals, pattern recognition, pitch content, real-time frame-based classification, rhythmic content, rhythmic structure, signal classification, statistical analysis, statistical pattern recognition classifiers training, timbral texture, whole file classification},
	pages = {293--302}
}

@article{roma_content-based_2010,
	title = {{CONTENT}-{BASED} {RETRIEVAL} {FROM} {UNSTRUCTURED} {AUDIO} {DATABASES} {USING} {AN} {ECOLOGICAL} {ACOUSTICS} {TAXONOMY}},
	abstract = {In this paper we describe a method to search for environmental sounds in unstructured databases with user-submitted material. The goal of the project is to facilitate the design of soundscapes in virtual environments. We analyze the use of a Support Vector Machine (SVM) as a learning algorithm to classify sounds according to a general sound events taxonomy based on ecological acoustics. In our experiments, we obtain accuracies above 80\% using crossvalidation. Finally, we present a web prototype that integrates the classiﬁer to rank sounds according to their relation to the taxonomy concepts.},
	language = {en},
	author = {Roma, Gerard and Janer, Jordi and Kersten, Stefan and Schirosa, Mattia and Herrera, Perfecto},
	year = {2010},
	pages = {6}
}

@article{chung_audio_2016,
	title = {Audio {Word}2Vec: {Unsupervised} {Learning} of {Audio} {Segment} {Representations} using {Sequence}-to-sequence {Autoencoder}},
	shorttitle = {Audio {Word}2Vec},
	url = {http://arxiv.org/abs/1603.00982},
	abstract = {The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word2Vec. It offers the vector representations of fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Audoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence. The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is furthered proposed offering more robust learning.},
	urldate = {2019-03-26TZ},
	journal = {arXiv:1603.00982 [cs]},
	author = {Chung, Yu-An and Wu, Chao-Chung and Shen, Chia-Hao and Lee, Hung-Yi and Lee, Lin-Shan},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.00982},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound}
}

@inproceedings{malkin_classifying_2005,
	title = {Classifying user environment for mobile applications using linear autoencoding of ambient audio},
	volume = {5},
	doi = {10.1109/ICASSP.2005.1416352},
	abstract = {Many mobile devices and applications can act in context-sensitive ways, but rely on explicit human action for context awareness. It would be preferable if our devices were able to attain context awareness without human intervention. One important aspect of user context is environment. We present a novel method for classifying environment types based on acoustic signals. This method makes use of linear autoencoding neural networks, and is motivated by the observation that biological coding systems seem to be heavily influenced by the statistics of their environments. We show that the autoencoder method achieved a lower error rate than a standard Gaussian mixture model on a representative sample task, and that a linear combination of autoencoders and GMMs yielded better performance than either alone.},
	booktitle = {Proceedings. ({ICASSP} '05). {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}, 2005.},
	author = {Malkin, R. G. and Waibel, A.},
	month = mar,
	year = {2005},
	keywords = {Acoustic devices, Biological system modeling, Context awareness, GMM, Gaussian mixture model, Gaussian processes, Humans, Interactive systems, Laboratories, Neural networks, Nonlinear filters, Statistics, Switches, acoustic signals, ambient audio encoding, audio coding, audio signal classification, biological coding systems, cellphone, context awareness, context-sensitivity, error rate, error statistics, linear autoencoding, linear autoencoding neural networks, linear codes, mobile applications, mobile devices, mobile handsets, neural nets, signal classification, user environment classification},
	pages = {v/509--v/512 Vol. 5}
}

@inproceedings{and_improve_2004,
	title = {Improve audio representation by using feature structure patterns},
	volume = {4},
	doi = {10.1109/ICASSP.2004.1326834},
	abstract = {Although statistical characteristics of audio features are widely used for audio representation in most current audio analysis systems and have been proved to be effective, they only utilize the average feature variations over time, and thus lead to ambiguities in some cases. Structure patterns, which describe the representative structure characteristics of both temporal and spectral features, are proposed to improve audio representation. In this paper, three structure patterns, including energy envelope pattern, sub-band spectral shape pattern and harmonicity prominence pattern, are proposed or refined, as successive development of our previous work. Evaluations on a content-based audio retrieval system with more than 1500 clips showed very encouraging results.},
	booktitle = {2004 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	author = {{and} and {and}},
	month = may,
	year = {2004},
	keywords = {Asia, Computer science, Content based retrieval, Frequency, Humans, Image analysis, Performance analysis, Spectral shape, Spectrogram, Statistical analysis, audio databases, audio representation, audio signal processing, content-based audio retrieval system, content-based retrieval, energy envelope pattern, feature extraction, feature structure patterns, harmonicity prominence pattern, signal representation, spectral analysis, spectral features, sub-band spectral shape pattern, temporal features},
	pages = {iv--iv}
}

@inproceedings{faller_efficient_2001,
	title = {Efficient representation of spatial audio using perceptual parametrization},
	doi = {10.1109/ASPAA.2001.969577},
	abstract = {We introduce a new scheme for simultaneous placement of a number of sources in auditory space. The scheme is based on an assumption about the relevance of localization cues in different critical bands. Given the sum signal of a number of sources, i.e. a monophonic signal, and a set of parameters (side-information) the scheme is capable of generating a binaural signal by spatially placing the sources contained in the monophonic signal. Potential applications for the scheme are multi-talker desktop conferencing and audio coding. Preliminary experimental results suggest that the listener's ability to identify messages in a multi-talker environment significantly improves by enhancing a monophonic signal with the proposed scheme.},
	booktitle = {Proceedings of the 2001 {IEEE} {Workshop} on the {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({Cat}. {No}.01TH8575)},
	author = {Faller, C. and Baumgarte, F.},
	month = oct,
	year = {2001},
	keywords = {Audio coding, Audio recording, Decoding, Ear, Headphones, Magnetic heads, Signal generators, Signal processing, Signal synthesis, Transfer functions, audio coding, auditory space, binaural signal, efficient spatial audio representation, localization cues, monophonic signal, multi-talker desktop conferencing, multi-talker environment, perceptual parametrization, side-information, signal representation, sum signal, teleconferencing},
	pages = {199--202}
}

@inproceedings{piczak_environmental_2015,
	title = {Environmental sound classification with convolutional neural networks},
	doi = {10.1109/MLSP.2015.7324337},
	abstract = {This paper evaluates the potential of convolutional neural networks in classifying short audio clips of environmental sounds. A deep model consisting of 2 convolutional layers with max-pooling and 2 fully connected layers is trained on a low level representation of audio data (segmented spectrograms) with deltas. The accuracy of the network is evaluated on 3 public datasets of environmental and urban recordings. The model outperforms baseline implementations relying on mel-frequency cepstral coefficients and achieves results comparable to other state-of-the-art approaches.},
	booktitle = {2015 {IEEE} 25th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Piczak, K. J.},
	month = sep,
	year = {2015},
	keywords = {Accuracy, Convolution, Convolutional codes, Neural networks, Pattern recognition, Training, Yttrium, audio clip, audio data, audio signal processing, baseline implementation, cepstral analysis, classification, convolutional layer, convolutional neural network, convolutional neural networks, environmental recording, environmental sound, environmental sound classification, low level representation, max-pooling, mel-frequency cepstral coefficient, neural nets, public dataset, segmented spectrogram, signal classification, urban recording},
	pages = {1--6}
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-03-26TZ},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound}
}

@article{phan_robust_2016,
	title = {Robust {Audio} {Event} {Recognition} with 1-{Max} {Pooling} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1604.06338},
	abstract = {We present in this paper a simple, yet efficient convolutional neural network (CNN) architecture for robust audio event recognition. Opposing to deep CNN architectures with multiple convolutional and pooling layers topped up with multiple fully connected layers, the proposed network consists of only three layers: convolutional, pooling, and softmax layer. Two further features distinguish it from the deep architectures that have been proposed for the task: varying-size convolutional filters at the convolutional layer and 1-max pooling scheme at the pooling layer. In intuition, the network tends to select the most discriminative features from the whole audio signals for recognition. Our proposed CNN not only shows state-of-the-art performance on the standard task of robust audio event recognition but also outperforms other deep architectures up to 4.5\% in terms of recognition accuracy, which is equivalent to 76.3\% relative error reduction.},
	urldate = {2019-03-26TZ},
	journal = {arXiv:1604.06338 [cs]},
	author = {Phan, Huy and Hertel, Lars and Maass, Marco and Mertins, Alfred},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06338},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound}
}

@inproceedings{xu_large-scale_2018,
	title = {Large-{Scale} {Weakly} {Supervised} {Audio} {Classification} {Using} {Gated} {Convolutional} {Neural} {Network}},
	doi = {10.1109/ICASSP.2018.8461975},
	abstract = {In this paper, we present a gated convolutional neural network and a temporal attention-based localization method for audio classification, which won the 1st place in the large-scale weakly supervised sound event detection task of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 challenge. The audio clips in this task, which are extracted from YouTube videos, are manually labelled with one or more audio tags, but without time stamps of the audio events, hence referred to as weakly labelled data. Two subtasks are defined in this challenge including audio tagging and sound event detection using this weakly labelled data. We propose a convolutional recurrent neural network (CRNN) with learnable gated linear units (GLUs) non-linearity applied on the log Mel spectrogram. In addition, we propose a temporal attention method along the frames to predict the locations of each audio event in a chunk from the weakly labelled data. The performances of our systems were ranked the 1st and the 2nd as a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6\% and Equal error 0.73, respectively.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Xu, Y. and Kong, Q. and Wang, W. and Plumbley, M. D.},
	month = apr,
	year = {2018},
	keywords = {Audio recording, Convolutional neural networks, DCASE 2017 challenge, DCASE2017 challenge, Event detection, Logic gates, Tagging, Task analysis, acoustic signal processing, attention, audio clips, audio event, audio signal processing, audio tagging, audio tags, convolution, convolutional recurrent neural network, feature extraction, feedforward neural nets, gated convolutional neural network, gated linear unit, large-scale weakly supervised sound event detection task, learnable gated linear units, learning (artificial intelligence), recurrent neural nets, signal classification, temporal attention method, temporal attention-based localization method, video signal processing, weakly labelled data, weakly supervised audio classification, weakly supervised sound event detection},
	pages = {121--125}
}

@article{lyon_machine_2010,
	title = {Machine {Hearing}: {An} {Emerging} {Field} [{Exploratory} {DSP}]},
	volume = {27},
	issn = {1053-5888},
	shorttitle = {Machine {Hearing}},
	doi = {10.1109/MSP.2010.937498},
	abstract = {If we had machines that could hear as humans do, we would expect them to be able to easily distinguish speech from music and background noises, to pull out the speech and music parts for special treatment, to know what direction sounds are coming from, to learn which noises are typical and which are noteworthy. Hearing machines should be able to organize what they hear; learn names for recognizable objects, actions, events, places, musical styles, instruments, and speakers; and retrieve sounds by reference to those names. These machines should be able to listen and react in real time, to take appropriate action on hearing noteworthy events, to participate in ongoing activities, whether in factories, in musical performances, or in phone conversations.},
	number = {5},
	journal = {IEEE Signal Processing Magazine},
	author = {Lyon, R. F.},
	month = sep,
	year = {2010},
	keywords = {Auditory system, Feature extraction, Machine vision, Maximum likelihood detection, Music, Nonlinear filters, Speech, hearing, machine hearing},
	pages = {131--139}
}

@inproceedings{lyon_sparse_2011,
	title = {Sparse coding of auditory features for machine hearing in interference},
	doi = {10.1109/ICASSP.2011.5947698},
	abstract = {A key problem in using the output of an auditory model as the input to a machine-learning system in a machine-hearing application is to find a good feature-extraction layer. For systems such as PAMIR (passive-aggressive model for image retrieval) that work well with a large sparse feature vector, a conversion from auditory images to sparse features is needed. For audio-file ranking and retrieval from text queries, based on stabilized auditory images, we took a multi-scale approach, using vector quantization to choose one sparse feature in each of many overlapping regions of different scales, with the hope that in some regions the features for a sound would be stable even when other interfering sounds were present and affecting other regions. We recently extended our testing of this approach using sound mixtures, and found that the sparse-coded auditory-image features degrade less in interference than vector-quantized MFCC sparse features do. This initial success suggests that our hope of robustness in interference may indeed be realizable, via the general idea of sparse features that are localized in a domain where signal components tend to be localized or stable.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Lyon, R. F. and Ponte, J. and Chechik, G.},
	month = may,
	year = {2011},
	keywords = {Auditory image, Encoding, Interference, Mel frequency cepstral coefficient, PAMIR, Sparse matrices, Testing, Training, Vector quantization, audio file ranking, feature extraction, image retrieval, learning (artificial intelligence), machine hearing, machine learning system, passive-aggressive model for image retrieval, sound mixture, sound retrieval and ranking, sparse code, sparse coded auditory image feature, sparse coding, vector quantisation, vector quantization},
	pages = {5876--5879}
}

@inproceedings{schuller_acoustic_2009,
	title = {Acoustic emotion recognition: {A} benchmark comparison of performances},
	shorttitle = {Acoustic emotion recognition},
	doi = {10.1109/ASRU.2009.5372886},
	abstract = {In the light of the first challenge on emotion recognition from speech we provide the largest-to-date benchmark comparison under equal conditions on nine standard corpora in the field using the two pre-dominant paradigms: modeling on a frame-level by means of hidden Markov models and supra-segmental modeling by systematic feature brute-forcing. Investigated corpora are the ABC, AVIC, DES, EMO-DB, eNTERFACE, SAL, SmartKom, SUSAS, and VAM databases. To provide better comparability among sets, we additionally cluster each database's emotions into binary valence and arousal discrimination tasks. In the result large differences are found among corpora that mostly stem from naturalistic emotions and spontaneous speech vs. more prototypical events. Further, supra-segmental modeling proves significantly beneficial on average when several classes are addressed at a time.},
	booktitle = {2009 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} {Understanding}},
	author = {Schuller, B. and Vlasenko, B. and Eyben, F. and Rigoll, G. and Wendemuth, A.},
	month = nov,
	year = {2009},
	keywords = {ABC databases, AVIC databases, Benchmark testing, Communication standards, DES databases, EMO-DB databases, Emotion recognition, Hidden Markov models, Man machine systems, Pattern recognition, Prototypes, SAL databases, SUSAS databases, SmartKom databases, Spatial databases, Speech analysis, Speech recognition, VAM databases, acoustic emotion recognition, eNTERFACE databases, emotion recognition, hidden Markov models, systematic feature brute-forcing},
	pages = {552--557}
}

@inproceedings{kohlsdorf_underwater_2013,
	address = {Zurich, Switzerland},
	title = {An underwater wearable computer for two way human-dolphin communication experimentation},
	isbn = {978-1-4503-2127-3},
	url = {http://dl.acm.org/citation.cfm?doid=2493988.2494346},
	doi = {10.1145/2493988.2494346},
	language = {en},
	urldate = {2019-03-10TZ},
	booktitle = {Proceedings of the 17th annual international symposium on {International} symposium on wearable computers - {ISWC} '13},
	publisher = {ACM Press},
	author = {Kohlsdorf, Daniel and Gilliland, Scott and Presti, Peter and Starner, Thad and Herzing, Denise},
	year = {2013},
	pages = {147}
}

@phdthesis{lehr_housing_2012,
	type = {thesis},
	title = {{HOUSING} {POLICY} {AND} {SOCIO}-{ECONOMIC} {RESIDENTIAL} {SEGREGATION}: {THE} {CASE} {OF} {BUENOS} {AIRES}, {ARGENTINA}},
	shorttitle = {{HOUSING} {POLICY} {AND} {SOCIO}-{ECONOMIC} {RESIDENTIAL} {SEGREGATION}},
	url = {https://repository.library.georgetown.edu/handle/10822/557553},
	abstract = {Current and past housing policy in Buenos Aires has not been effective for the social insertion of the poor, because, by focusing primarily on the provision of housing, it has not attempted to reduce residential segregation and mitigate the consequences of living in a poor neighborhood. Despite inclusive urban policy aiming to create a balanced distribution of social classes throughout the city and the prioritization of pro-poor housing options from 1900-mid 1950s, Argentina has been experiencing polarizing tendencies since the 1960s, these becoming more acute in the 1980s and 90s. As a result, residential segregation in the city has increased and a housing shortage has persisted. In response to this situation, the state's general housing policy response throughout Argentina has focused on the construction of public housing complexes, which has contributed to the poor's urban isolation by concentrating poverty in undesirable neighborhoods. Lessons on more inclusive housing options can be taken from the United States, and select European and Latin American countries, but desegregation policies are extremely immature throughout the world. This investigation constitutes a state of the art study, in which extensive research was conducted to reconstruct current and historical housing and urban policy in Buenos Aires, in order to provide a stepping stone for more empirical work on this topic in the region.},
	language = {en},
	urldate = {2019-03-06TZ},
	school = {Georgetown University},
	author = {Lehr, Lindsay},
	year = {2012}
}

@article{de_nadai_economic_2018,
	title = {The economic value of neighborhoods: {Predicting} real estate prices from the urban environment},
	shorttitle = {The economic value of neighborhoods},
	url = {http://arxiv.org/abs/1808.02547},
	abstract = {Housing costs have a significant impact on individuals, families, businesses, and governments. Recently, online companies such as Zillow have developed proprietary systems that provide automated estimates of housing prices without the immediate need of professional appraisers. Yet, our understanding of what drives the value of houses is very limited. In this paper, we use multiple sources of data to entangle the economic contribution of the neighborhood's characteristics such as walkability and security perception. We also develop and release a framework able to now-cast housing prices from Open data, without the need for historical transactions. Experiments involving 70,000 houses in 8 Italian cities highlight that the neighborhood's vitality and walkability seem to drive more than 20\% of the housing value. Moreover, the use of this information improves the nowcast by 60\%. Hence, the use of property's surroundings' characteristics can be an invaluable resource to appraise the economic and social value of houses after neighborhood changes and, potentially, anticipate gentrification.},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {arXiv:1808.02547 [cs]},
	author = {De Nadai, Marco and Lepri, Bruno},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.02547},
	keywords = {Computer Science - Computers and Society}
}

@article{liu_hedonic_2017,
	title = {Hedonic {Price} {Modeling} of {New} {Residential} {Property} {Values} in {Xi}’an {City}, {China}},
	volume = {5},
	copyright = {Copyright (c) 2017 International Journal of Social Science Studies},
	issn = {2324-8041},
	url = {http://redfame.com/journal/index.php/ijsss/article/view/2510},
	doi = {10.11114/ijsss.v5i9.2510},
	abstract = {This study analyzed new residential property values of Xi’an City in March, 2014. Results show that accessibility indices, such as distance to CBD, have been well capitalized into the residential property values. Particularly, a within-zone housing unit is sold 9.4\% more than if it was outside the attendance zone of a Key Primary School, i.e. home buyers have how much willingness-to-pay for the high-quality educational resource. Another corollary is got that the accessibility to subway stations has not significantly been capitalized, only with a low premium reflecting in the real estate market of Xi’an City. Considered that spatial local singularities caused by unobserved variables or estimation bias can be associated with multi-regression errors, this study herein has explored an unconventional viewpoint to residual problem, which combines the regional differences (coming from real world) and the spatial distributions of singularities (feedback from data). Furthermore, whole samples are classified into 5 agglomerations for revealing the underlying reasons about the future trend and variation of real estate market within each region. It is effective to provide scientific basis of decision making for the real estate investors and planners.},
	language = {en},
	number = {9},
	urldate = {2019-03-04TZ},
	journal = {International Journal of Social Science Studies},
	author = {Liu, Kai and Ichinose, Toshiaki},
	month = sep,
	year = {2017},
	pages = {42--56}
}

@article{noauthor_argentinas_2017,
	title = {Argentina's {Macri} boosts mortgage credits to win back middle class},
	url = {https://www.reuters.com/article/argentina-economy-mortgages-idUSL1N1K5008},
	abstract = {Argentina's new subsidized mortgage scheme is gathering steam, boosting con...},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {Reuters},
	month = jul,
	year = {2017},
	keywords = {ARGENTINA, Americas, Argentina, Backed Markets, Banking Services (Legacy), Banking Services (TRBC), Banking and Investment Services (TRBC), Banks (TRBC), Construction and Engineering (TRBC), Consumer Financial Services (TRBC), Corporate Events, Credit and Corporate Debt, Cristina Fernandez, Debt / Fixed Income Markets, ECONOMY/MORTGAGES, Economic Events, Elections / Voting, Emerging Market Countries, Financials (Legacy), Financials (TRBC), General News, Government / Politics, Industrial Services (TRBC), Industrials (TRBC), Inflation, Ivan Kerr, Juan Curutchet, Loans, MARIEL FORNONI, Mauricio Macri, Mortgage, Real Estate Markets, South America / Central America}
}

@misc{noauthor_buenos_nodate,
	title = {Buenos {Aires} {Ciudad} - {Gobierno} de la {Ciudad} {Autónoma} de {Buenos} {Aires} {\textbar}},
	url = {https://www.buenosaires.gob.ar/},
	urldate = {2019-03-04TZ}
}

@misc{aristaran_ejercicio_2018,
	title = {Ejercicio de visualización de los datos de recorridos de las bicicletas públicas de {Buenos} {Aires}: jazzido/vizbici},
	copyright = {MIT},
	shorttitle = {Ejercicio de visualización de los datos de recorridos de las bicicletas públicas de {Buenos} {Aires}},
	url = {https://github.com/jazzido/vizbici},
	urldate = {2019-03-04TZ},
	author = {Aristarán, Manuel},
	month = jul,
	year = {2018},
	note = {original-date: 2013-05-11T19:36:59Z}
}

@misc{aleph_0_this_2018,
	title = {This project creates a map of the income distribution for {Buenos} {Aires} {City} usingo 2010 census, with {RADIOS} as areabreaks and using {CAPECO} and {CONHAB} indexes as proxy for income: alephcero/{incomeM}..},
	copyright = {GPL-2.0},
	shorttitle = {This project creates a map of the income distribution for {Buenos} {Aires} {City} usingo 2010 census, with {RADIOS} as areabreaks and using {CAPECO} and {CONHAB} indexes as proxy for income},
	url = {https://github.com/alephcero/incomeMapBuenosAires},
	urldate = {2019-03-04TZ},
	author = {Aleph 0},
	month = jun,
	year = {2018},
	note = {original-date: 2015-07-27T18:21:51Z}
}

@misc{noauthor_properati_nodate,
	title = {Properati {Data}},
	url = {https://www.properati.com.ar/data},
	urldate = {2019-03-04TZ}
}

@article{agarwal_impact_2005,
	title = {The impact of the 2001 financial crisis and the economic policy responses on the {Argentine} mortgage market},
	volume = {14},
	issn = {10511377},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1051137705000215},
	doi = {10.1016/j.jhe.2005.07.001},
	language = {en},
	number = {3},
	urldate = {2019-03-04TZ},
	journal = {Journal of Housing Economics},
	author = {Agarwal, Sumit and Chomsisengphet, Souphala and Hassler, Olivier},
	month = sep,
	year = {2005},
	pages = {242--270}
}

@article{baldominos_identifying_2018,
	title = {Identifying {Real} {Estate} {Opportunities} using {Machine} {Learning}},
	volume = {8},
	issn = {2076-3417},
	url = {http://arxiv.org/abs/1809.04933},
	doi = {10.3390/app8112321},
	abstract = {The real estate market is exposed to many ﬂuctuations in prices because of existing correlations with many variables, some of which cannot be controlled or might even be unknown. Housing prices can increase rapidly (or in some cases, also drop very fast), yet the numerous listings available online where houses are sold or rented are not likely to be updated that often. In some cases, individuals interested in selling a house (or apartment) might include it in some online listing, and forget about updating the price. In other cases, some individuals might be interested in deliberately setting a price below the market price in order to sell the home faster, for various reasons. In this paper, we aim at developing a machine learning application that identiﬁes opportunities in the real estate market in real time, i.e., houses that are listed with a price substantially below the market price. This program can be useful for investors interested in the housing market. We have focused in a use case considering real estate assets located in the Salamanca district in Madrid (Spain) and listed in the most relevant Spanish online site for home sales and rentals. The application is formally implemented as a regression problem that tries to estimate the market price of a house given features retrieved from public online listings. For building this application, we have performed a feature engineering stage in order to discover relevant features that allows for attaining a high predictive performance. Several machine learning algorithms have been tested, including regression trees, k-nearest neighbors, support vector machines and neural networks, identifying advantages and handicaps of each of them.},
	language = {en},
	number = {11},
	urldate = {2019-03-04TZ},
	journal = {Applied Sciences},
	author = {Baldominos, Alejandro and Blanco, Iván and Moreno, Antonio José and Iturrarte, Rubén and Bernárdez, Óscar and Afonso, Carlos},
	month = nov,
	year = {2018},
	note = {arXiv: 1809.04933},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
	pages = {2321}
}

@article{blanco_socio-territorial_2018,
	title = {Socio-territorial inequality and differential mobility. {Three} key issues in the {Buenos} {Aires} {Metropolitan} {Region}},
	volume = {67},
	issn = {09666923},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0966692316303623},
	doi = {10.1016/j.jtrangeo.2017.07.008},
	abstract = {One of the main challenges that geographers and urban planners face when thinking about mobility in Latin American cities is how to accurately assess the eﬀect produced by severe social and territorial inequality. In an attempt to explore this question, this article analyses three key issues related to the inequality-mobility relationship: a) mobility as a facilitator in the access to goods, services and opportunities at diﬀerent urban scales, and its direct eﬀects on poverty and social exclusion; b) socially and territorially conditioned assets and competences among individuals when managing mobility needs and territorial control; and c) the uneven appropriation and use of the city, both in terms of proximity and connection to metropolitan networks. Analysis is carried out on secondary information on transport and mobility at the metropolitan scale according to income level and territorial location of households. This is followed by examination of three speciﬁc cases in the Buenos Aires Metropolitan Region that show the importance of territorial features when addressing mobility patterns of particular socioeconomically vulnerable groups, including: mobility of informal settlers in urban peripheries; mobility of domestic workers in gated communities; and mobility of residents at risk of displacement in gentrifying neighborhoods. The key ﬁndings highlight how the particular territorial conditions can intensify or attenuate the pre-existing socioeconomic inequality.},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {Journal of Transport Geography},
	author = {Blanco, Jorge and Apaolaza, Ricardo},
	month = feb,
	year = {2018},
	pages = {76--84}
}

@misc{noauthor_usd_nodate,
	title = {{USD} {ARS} {Historical} {Data}},
	url = {https://www.investing.com/currencies/usd-ars-historical-data},
	abstract = {Get free historical data for the USD ARS (US Dollar Argentinian Peso) currency pair, viewable in daily, weekly or monthly time intervals.},
	language = {en-us},
	urldate = {2019-03-04TZ},
	journal = {Investing.com}
}

@article{xu_effect_2012,
	title = {The effect of monetary policy on real estate price growth in {China}},
	volume = {20},
	issn = {0927538X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0927538X1100045X},
	doi = {10.1016/j.pacfin.2011.08.001},
	abstract = {Using quarterly data from 1998:Q1 to 2009:Q4 and monthly data from July 2005 to February 2010, this paper examines the impact of key monetary policy variables, including long-term benchmark bank loan rate, money supply growth, and mortgage credit policy indicator, on the real estate price growth dynamics in China. Empirical results consistently demonstrate that expansionary monetary policy tends to accelerate the subsequent home price growth, while restrictive monetary policy tends to decelerate the subsequent home price growth. These results suggest that Chinese monetary policy actions are the key driving forces behind the change of real estate price growth in China. We also show that hot money ﬂow does not have a signiﬁcant impact on the change of home price growth after controlling for the money supply growth. Finally, a bullish stock market tends to accelerate subsequent home price growth.},
	language = {en},
	number = {1},
	urldate = {2019-03-04TZ},
	journal = {Pacific-Basin Finance Journal},
	author = {Xu, Xiaoqing Eleanor and Chen, Tao},
	month = jan,
	year = {2012},
	pages = {62--77}
}

@article{camacho_toward_2015,
	title = {Toward a more reliable picture of the economic activity: {An} application to {Argentina}},
	volume = {132},
	issn = {01651765},
	shorttitle = {Toward a more reliable picture of the economic activity},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165176515001330},
	doi = {10.1016/j.econlet.2015.03.032},
	abstract = {We advocate a dynamic factor model to provide alternative measures of output data using indirect information from economic indicators. We apply the method to show evidence of a significant gap between estimated and official measures of Argentine GDP since 2007.},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {Economics Letters},
	author = {Camacho, Maximo and Dal Bianco, Marcos and Martinez-Martin, Jaime},
	month = jul,
	year = {2015},
	pages = {129--132}
}

@article{thomas_argentinas_2016,
	title = {Argentina's post-2001 economy and the 2014 default},
	volume = {60},
	issn = {10629769},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1062976915000848},
	doi = {10.1016/j.qref.2015.08.002},
	abstract = {With two sovereign debt defaults within the last thirteen years, Argentina represents an interesting example of the causes and effects of defaults by sovereign states. Argentina has had a history of economic strife for the last one hundred years, largely due to political and economic instability. This paper discusses the economics of Argentina after the 2001 ﬁnancial crisis that led to the default in 2014. The paper also explains the main legal aspects for the holdouts versus the Argentina trial over the pari passu controversy and discusses the different reactions to the trial and consequent default.},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {The Quarterly Review of Economics and Finance},
	author = {Thomas, Carolyn and Cachanosky, Nicolás},
	month = may,
	year = {2016},
	pages = {70--80}
}

@article{cho_developing_2014,
	title = {Developing an amenity value calculator for urban forest landscapes},
	volume = {43},
	issn = {01989715},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0198971513000902},
	doi = {10.1016/j.compenvurbsys.2013.10.005},
	abstract = {The goal of this research is to develop a framework that can be used by landscape and urban planners to implement an ‘‘amenity value calculator’’ for urban forest landscapes across a metropolitan county. By balancing the pros and cons of using typical hedonic frameworks versus urban forest inventory and management software systems, we (1) construct a data-driven approach to estimate the total amenity value associated with access to, views of, and existence of a particular forest landscape from among all available forest sites in a community and (2) develop a framework for an amenity value calculator for numerous community forest landscapes within a metropolitan county, using the amenity values generated from objective (1), that can be accessed and understood by anyone who is interested in the beneﬁts provided by nearby community forests. Our research suggests that (i) residential household’s amenity value per acre of forest landscape decreases asymptotically towards zero as the driving time from a residential house increases, (ii) an amenity value calculator can be developed to sum the amenity values across all detached single-family houses within a range of driving times from any selected forest landscape, and (iii) a user-friendly, web-based application, that allows users to view the estimated amenity values of forest landscapes that interest them, can be created to better inform the public about the values of forest landscapes of interest to them.},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {Computers, Environment and Urban Systems},
	author = {Cho, Seong-Hoon and Kim, Taeyoung and Roberts, Roland K. and Hellwinckel, Chad and Kim, Seung Gyu and Wilson, Brad},
	month = jan,
	year = {2014},
	pages = {34--41}
}

@article{cheshire_price_1995,
	title = {On the {Price} of {Land} and the {Value} of {Amenities}},
	volume = {62},
	issn = {00130427},
	url = {https://www.jstor.org/stable/2554906?origin=crossref},
	doi = {10.2307/2554906},
	language = {en},
	number = {246},
	urldate = {2019-03-04TZ},
	journal = {Economica},
	author = {Cheshire, Paul and Sheppard, Stephen},
	month = may,
	year = {1995},
	pages = {247}
}

@article{fu_modeling_2016,
	title = {Modeling of {Geographic} {Dependencies} for {Real} {Estate} {Ranking}},
	volume = {11},
	issn = {15564681},
	url = {http://dl.acm.org/citation.cfm?doid=2974720.2934692},
	doi = {10.1145/2934692},
	language = {en},
	number = {1},
	urldate = {2019-03-04TZ},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Fu, Yanjie and Xiong, Hui and Ge, Yong and Zheng, Yu and Yao, Zijun and Zhou, Zhi-Hua},
	month = aug,
	year = {2016},
	pages = {1--27}
}

@article{di_virgilio_housing_2017,
	title = {Housing policy in {Argentina}: reflections on a decade of progressive social policy},
	volume = {17},
	issn = {1949-1247, 1949-1255},
	shorttitle = {Housing policy in {Argentina}},
	url = {https://www.tandfonline.com/doi/full/10.1080/19491247.2016.1266868},
	doi = {10.1080/19491247.2016.1266868},
	language = {en},
	number = {2},
	urldate = {2019-03-04TZ},
	journal = {International Journal of Housing Policy},
	author = {Di Virgilio, María Mercedes},
	month = apr,
	year = {2017},
	pages = {314--325}
}

@inproceedings{de_nadai_are_2016,
	address = {Amsterdam, The Netherlands},
	title = {Are {Safer} {Looking} {Neighborhoods} {More} {Lively}?: {A} {Multimodal} {Investigation} into {Urban} {Life}},
	isbn = {978-1-4503-3603-1},
	shorttitle = {Are {Safer} {Looking} {Neighborhoods} {More} {Lively}?},
	url = {http://dl.acm.org/citation.cfm?doid=2964284.2964312},
	doi = {10.1145/2964284.2964312},
	language = {en},
	urldate = {2019-03-04TZ},
	booktitle = {Proceedings of the 2016 {ACM} on {Multimedia} {Conference} - {MM} '16},
	publisher = {ACM Press},
	author = {De Nadai, Marco and Vieriu, Radu Laurentiu and Zen, Gloria and Dragicevic, Stefan and Naik, Nikhil and Caraviello, Michele and Hidalgo, Cesar Augusto and Sebe, Nicu and Lepri, Bruno},
	year = {2016},
	pages = {1127--1135}
}

@article{li_analyzing_2018,
	title = {Analyzing housing prices in {Shanghai} with open data: {Amenity}, accessibility and urban structure},
	issn = {02642751},
	shorttitle = {Analyzing housing prices in {Shanghai} with open data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0264275118305936},
	doi = {10.1016/j.cities.2018.11.016},
	abstract = {Skyrocketing housing prices in China's megacities have generated broad concerns. By integrating open data from Lianjia.com, Dianping.com, Mobike.com, and Baidu Map POI, we analyze spatial patterns of apartment prices and their association with local attributes in Shanghai. We ﬁnd that Shanghai's residential market still has a monocentric structure because of the centralized distribution of public transport facilities and amenities. Hedonic models further conﬁrm that structural attributes, accessibility, as well as public and private service amenities signiﬁcantly shape the real estate market. These factors also are diﬀerentiated so as to form a pattern of concentric rings. In the inner-city and expanded inner-city areas, public service amenities such as parks, schools, hospitals, and banks, as well as private service amenities such as entertainment, shopping, and residential service facilities, boost housing prices. In the suburbs, better access to bike sharing, bus stops, and metro stations are the top preferences for apartment buyers. Our study also indicates that the Chinese government needs to make public and private services more accessible, not only spatially in urban peripheries and villages, but also institutionally to lower income families who cannot aﬀord apartments in expensive neighborhoods.},
	language = {en},
	urldate = {2019-03-04TZ},
	journal = {Cities},
	author = {Li, Han and Wei, Yehua Dennis and Wu, Yangyi and Tian, Guang},
	month = nov,
	year = {2018}
}

@article{wernau_argentinas_2018,
	chapter = {Markets},
	title = {Argentina’s {Peso} {Plunges} to {Record} {Low}},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/argentinas-peso-plunges-to-record-low-1535578085},
	abstract = {The currency fell to a record low against the dollar Wednesday after President Mauricio Macri announced he has asked the International Monetary Fund to speed up delivery of a \$50 billion bailout package.},
	language = {en-US},
	urldate = {2019-03-02TZ},
	journal = {Wall Street Journal},
	author = {Wernau, Julie},
	month = aug,
	year = {2018},
	keywords = {Mauricio Macri, argentina currency crisis, argentine currency, argentine peso, commodity, currency markets, economic news, financial market news, foreign exchange markets, general news, government borrowing, government finance, international relations, money, political, politics}
}

@article{guerra_residential_2018,
	title = {Residential location, urban form, and household transportation spending in {Greater} {Buenos} {Aires}},
	volume = {72},
	issn = {0966-6923},
	url = {https://trid.trb.org/view/1540595},
	number = {0},
	urldate = {2019-03-01TZ},
	journal = {Journal of Transport Geography},
	author = {Guerra, Erick and Caudillo, Camilo and Goytia, Cynthia and Quiros, Tatiana Peralta and Rodriguez, Camila},
	month = oct,
	year = {2018}
}

@book{lyon_human_2017,
	edition = {1},
	title = {Human and {Machine} {Hearing}: {Extracting} {Meaning} from {Sound}},
	isbn = {978-1-107-00753-6 978-1-139-05169-9},
	shorttitle = {Human and {Machine} {Hearing}},
	url = {https://www.cambridge.org/core/product/identifier/9781139051699/type/book},
	language = {en},
	urldate = {2019-01-22TZ},
	publisher = {Cambridge University Press},
	author = {Lyon, Richard F.},
	month = may,
	year = {2017},
	doi = {10.1017/9781139051699}
}

@article{Roma2010,
	title = {Ecological acoustics perspective for content-based retrieval of environmental sounds},
	volume = {2010},
	issn = {16874714},
	url = {http://asmp.eurasipjournals.com/content/2010/1/960863},
	doi = {10.1155/2010/960863},
	abstract = {In this paper we present a method to search for environmental sounds in large unstructured databases of user-submitted audio, using a general sound events taxonomy from ecological acoustics.We discuss the use of Support VectorMachines to classify sound recordings according to the taxonomy and describe two use cases for the obtained classificationmodels: a content-basedweb search interface for a large audio database and a method for segmenting field recordings to assist sound design.},
	urldate = {2018-09-22},
	journal = {Eurasip Journal on Audio, Speech, and Music Processing},
	author = {Roma, Gerard and Janer, Jordi and Kersten, Stefan and Schirosa, Mattia and Herrera, Perfecto and Serra, Xavier},
	year = {2010},
	note = {Publisher: Hindawi Publishing Corp.
ISBN: 1687-4714},
	pages = {1--11}
}

@inproceedings{Kim2009,
	title = {Acoustic topic model for audio information retrieval},
	isbn = {978-1-4244-3679-8},
	url = {http://ieeexplore.ieee.org/document/5346483/},
	doi = {10.1109/ASPAA.2009.5346483},
	abstract = {A new algorithm for content-based audio information retrieval is introduced in this work. Assuming that there exist hidden acoustic topics and each audio clip is a mixture of those acoustic topics, we proposed a topic model that learns a probability distribution over a set of hidden topics of a given audio clip in an unsupervised manner. We use the Latent Dirichlet Allocation (LDA) method for the topic model, and introduce the notion of acoustic words for supporting modeling within this framework. In audio description classification tasks using Support Vector Machine (SVM) on the BBC database, the proposed acoustic topic model shows promising results by outperforming the Latent Perceptual Indexing (LPI) method in classifying onomatopoeia descriptions and semantic descriptions.},
	urldate = {2018-09-22},
	booktitle = {{IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}},
	publisher = {IEEE},
	author = {Kim, Samuel and Narayanan, Shrikanth and Sundaram, Shiva},
	month = oct,
	year = {2009},
	note = {ISSN: 1931-1168},
	pages = {37--40}
}

@article{Berenzweig2004,
	title = {A large-scale evaluation of acoustic and subjective music-similarity measures},
	volume = {28},
	issn = {01489267},
	url = {http://www.mitpressjournals.org/doi/10.1162/014892604323112257},
	doi = {10.1162/014892604323112257},
	abstract = {A valuable goal in the field of Music Information Retrieval (MIR) is to devise an automatic measure of the similarity between two musical recordings based only on an analysis of their audio content. Such a tool—a quantitative measure of similarity— can be used to build classification, retrieval, browsing, and recommendation systems. To develop such a measure, however, presupposes some ground truth, a single underlying similarity that constitutes the desired output of the measure. Music similarity is an elusive concept—wholly subjective, multifaceted, and a moving target—but one that must be pursued in support of applications to provide automatic organization of large music collections.},
	number = {2},
	urldate = {2018-09-22},
	journal = {Computer Music Journal},
	author = {Berenzweig, Adam and Logan, Beth and Ellis, Daniel P.W. and Whitman, Brian},
	month = jun,
	year = {2004},
	note = {ISBN: 0148-9267},
	keywords = {acoustic measures, evaluation, ground-truth, music similarity},
	pages = {63--76}
}
@article{rodriguez_city_2016,
	title = {A city for all? {Public} policy and resistance to gentrification in the southern neighborhoods of {Buenos} {Aires}},
	volume = {37},
	issn = {0272-3638},
	shorttitle = {A city for all?},
	url = {https://doi.org/10.1080/02723638.2016.1152844},
	doi = {10.1080/02723638.2016.1152844},
	abstract = {In this paper, we analyze grassroots movements’ resistance to gentrification processes in the southern area of Buenos Aires. We first review the limitations of the concept of gentrification when applied to the transformation of Latin American cities. We then examine the relationship between gentrification and social class in order to explain why and how local residents and grassroots organizations mount resistance to gentrification in three pericentral neighborhoods in the southern portion of Buenos Aires. Contemporary changes in these neighborhoods are driven by (1) the promotion of neoliberal urban renewal policies, and (2) the genesis and development of Law 341, a program that provides low-income people and organizations with loans for housing construction and renovation, and (3) the Programa de Autogestión de la Vivienda (the Self-Managed Housing Program), which supports cooperative-style housing management. Through two cases, we examine how the actions and strategies of grassroots organizations have countered some of the effects of gentrification in the South of Buenos Aires.},
	number = {8},
	urldate = {2019-03-01TZ},
	journal = {Urban Geography},
	author = {Rodríguez, María Carla and Virgilio, María Mercedes Di},
	month = nov,
	year = {2016},
	keywords = {Gentrification, grassroots organizations, neoliberal urbanism, resistance to gentrification, social class, urban social movements},
	pages = {1215--1234}
}

@article{borsdorf_social_2016,
	series = {Special {Issue}: {Urban} {Segregation} in {Latin} {America}},
	title = {Social segregation and gated communities in {Santiago} de {Chile} and {Buenos} {Aires}. {A} comparison},
	volume = {54},
	issn = {0197-3975},
	url = {http://www.sciencedirect.com/science/article/pii/S0197397515300424},
	doi = {10.1016/j.habitatint.2015.11.033},
	abstract = {The two Cono Sur capitals share similarities (effects of globalization, socio-spatial segregation, fragmentation, rise of gated cities, social housing in remote locations lacking public transport), however a variety of differences can be observed. Whereas in Santiago gated communities are widespread in the urban fabric, in Buenos Aires these communities are concentrated only in the north and northwestern parts of the city. In Chile parcelas de agrado with more than 5000 m² fulfill the demand of upper class citizens for more space for luxury villas. Ethnic segregation is seen as a problem in Santiago, where Peruvian immigrants occupy some sectors of the city centre, whereas in Buenos Aires most immigrants are coming from non-metropolitan regions of Argentina. Last not least gentrification in quite strong in Santiago’s central communes, whereas the Argentines in a quite unusual way interpret the development of gated communities as a process of social upgrading.},
	urldate = {2019-03-01TZ},
	journal = {Habitat International},
	author = {Borsdorf, Axel and Hildalgo, Rodrigo and Vidal-Koppmann, Sonia},
	month = may,
	year = {2016},
	pages = {18--27}
}

@misc{brian_mcfee_2018_1252297,
	title = {librosa/librosa: 0.6.1},
	url = {https://doi.org/10.5281/zenodo.1252297},
	author = {McFee, Brian and McVicar, Matt and Balke, Stefan and Thomé, Carl and Raffel, Colin and Lee, Dana and Nieto, Oriol and Battenberg, Eric and Ellis, Dan and Yamamoto, Ryuichi and Moore, Josh and Bittner, Rachel and Choi, Keunwoo and Friesch, Pius and Stöter, Fabian-Robert and Lostanlen, Vincent and Kumar, Siddhartha and Waloschek, Simon and Kranzler, Seth and Naktinis, Rimvydas and Repetto, Douglas and Hawthorne, Curtis ``Fjord'' and Carr, C J and Pimenta, Waldir and Viktorin, Petr and Brossier, Paul and Santos, João Felipe and Wu, Jackie and Peterson, Erik and Holovaty, Adrian},
	month = may,
	year = {2018},
	doi = {10.5281/zenodo.1252297}
}

@misc{chollet2015keras,
	title = {Keras},
	author = {Chollet, François and {others}},
	year = {2015},
	note = {Medium: {\textbackslash}url\{https://keras.io\}}
}

@book{Plack2018,
	address = {Third edition. {\textbar} Abingdon, Oxon; New York, NY: Routledge, 2018.},
	title = {The {Sense} of {Hearing}},
	isbn = {978-1-315-20814-5},
	url = {https://www.taylorfrancis.com/books/9781315208145},
	urldate = {2018-12-08},
	publisher = {Routledge},
	author = {Plack, Christopher J.},
	month = jun,
	year = {2018},
	doi = {10.4324/9781315208145}
}

@techreport{bishop1994mixture,
	title = {Mixture density networks},
	institution = {Citeseer},
	author = {Bishop, Christopher M},
	year = {1994}
}

@article{slaney1993importance,
	title = {On the importance of time-a temporal representation of sound},
	volume = {95116},
	journal = {Visual representations of speech signals},
	author = {Slaney, Malcolm and Lyon, Richard F},
	year = {1993},
	note = {Publisher: Chichester: Wiley}
}

@article{Piazza2013,
	title = {Humans {Use} {Summary} {Statistics} to {Perceive} {Auditory} {Sequences}},
	volume = {24},
	issn = {0956-7976},
	url = {http://journals.sagepub.com/doi/10.1177/0956797612473759},
	doi = {10.1177/0956797612473759},
	abstract = {In vision, humans use summary statistics (e.g., the average facial expression of a crowd) to efficiently perceive the gist of groups of features. Here, we present direct evidence that ensemble coding is also important for auditory processing. We found that listeners could accurately estimate the mean frequency of a set of logarithmically spaced pure tones presented in a temporal sequence (Experiment 1). Their performance was severely reduced when only a subset of tones from a given sequence was presented (Experiment 2), which demonstrates that ensemble coding is based on a substantial number of the tones in a sequence. This precise ensemble coding occurred despite very limited representation of individual tones from the sequence: Listeners were poor at identifying specific individual member tones (Experiment 3) and at determining their positions in the sequence (Experiment 4). Together, these results indicate that summary statistical coding is not limited to visual processing and is an important auditory mechanism for extracting ensemble frequency information from sequences of sounds.},
	number = {8},
	journal = {Psychological Science},
	author = {Piazza, Elise A. and Sweeny, Timothy D. and Wessel, David and Silver, Michael A. and Whitney, David},
	month = aug,
	year = {2013},
	pmid = {23761928},
	note = {ISBN: 1467-9280 (Electronic){\textbackslash}n0956-7976 (Linking)},
	keywords = {auditory perception, ensemble coding, frequency, perception, statistical summary, visual perception},
	pages = {1389--1397}
}

@article{Eggermont2001,
	title = {Between sound and perception: {Reviewing} the search for a neural code},
	volume = {157},
	issn = {03785955},
	doi = {10.1016/S0378-5955(01)00259-3},
	abstract = {This review investigates the roles of representation, transformation and coding as part of a hierarchical process between sound and perception. This is followed by a survey of how speech sounds and elements thereof are represented in the activity patterns along the auditory pathway. Then the evidence for a place representation of texture features of sound, comprising frequency, periodicity pitch, harmonicity in vowels, and direction and speed of frequency modulation, and for a temporal and synchrony representation of sound contours, comprising onsets, offsets, voice onset time, and low rate amplitude modulation, in auditory cortex is reviewed. Contours mark changes and transitions in sound and auditory cortex appears particularly sensitive to these dynamic aspects of sound. Texture determines which neurons, both cortical and subcortical, are activated by the sound whereas the contours modulate the activity of those neurons. Because contours are temporally represented in the majority of neurons activated by the texture aspects of sound, each of these neurons is part of an ensemble formed by the combination of contour and texture sensitivity. A multiplexed coding of complex sound is proposed whereby the contours set up widespread synchrony across those neurons in all auditory cortical areas that are activated by the texture of sound. Copyright © 2001 Elsevier Science B.V.},
	number = {1-2},
	journal = {Hearing Research},
	author = {Eggermont, Jos J.},
	year = {2001},
	pmid = {11470183},
	note = {ISBN: 0378-5955},
	keywords = {Amplitude and frequency modulation, Auditory system, Neural coding, Neural representation, Neural synchrony, Neural transformation, Speech, Vocalization, Voice onset time},
	pages = {1--42}
}

@article{Hickok2007,
	title = {The cortical organization of speech processing},
	volume = {8},
	issn = {1471-003X},
	url = {http://www.nature.com/articles/nrn2113},
	doi = {10.1038/nrn2113},
	abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized — although there are important computational differences between the left- and right-hemisphere systems — and that the dorsal stream is strongly left-hemisphere dominant.},
	number = {5},
	journal = {Nature Reviews Neuroscience},
	author = {Hickok, Gregory and Poeppel, David},
	month = may,
	year = {2007},
	pmid = {17431404},
	note = {arXiv: NIHMS150003
ISBN: 1471-003X (Print)},
	pages = {393--402}
}

@article{Choudhury2008,
	title = {The {Mobile} {Sensing} {Platform}: {An} {Embedded} {Activity} {Recognition} {System}},
	volume = {7},
	issn = {1536-1268},
	url = {http://ieeexplore.ieee.org/document/4487086/},
	doi = {10.1109/MPRV.2008.39},
	abstract = {The Mobile Sensing Platform (MSP) is a small-form-factor wearable device designed for embedded activity recognition. The MSP aims broadly to support context-aware ubiquitous computing applications. It incorporates multimodal sensing, data processing and inference, storage, all-day battery life, and wireless connectivity into a single 4 oz (115 g) wearable unit. Several design iterations and real-world deployments over the last four years have identified a set of core hardware and software requirements for a mobile inference system. This article presents findings and lessons learned in the course of designing, improving and using this system. This article is part of a special issue on activity-based computing.},
	number = {2},
	urldate = {2018-12-08},
	journal = {IEEE Pervasive Computing},
	author = {Choudhury, Tanzeem and Borriello, Gaetano and Consolvo, Sunny and Haehnel, Dirk and Harrison, Beverly and Hemingway, Bruce and Hightower, Jeffrey and Klasnja, Predrag "Pedja" and Koscher, Karl and LaMarca, Anthony and Landay, James A. and LeGrand, Louis and Lester, Jonathan and Rahimi, Ali and Rea, Adam and Wyatt, Danny},
	month = apr,
	year = {2008},
	note = {Publisher: IEEE Computer Society},
	keywords = {activity recognition, embedded systems, machine learning, wearable computers},
	pages = {32--41}
}

@article{Meyer2017,
	title = {Unsupervised {Feature} {Learning} for {Audio} {Analysis}},
	url = {http://arxiv.org/abs/1712.03835},
	abstract = {Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 \% better results when used with a classifier and 36 \% better results when used for clustering.},
	urldate = {2018-11-19},
	author = {Meyer, Matthias and Beutel, Jan and Thiele, Lothar},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.03835}
}

@article{Boyd2010,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {3},
	issn = {1935-8237},
	url = {http://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	number = {1},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Boyd, Stephen},
	year = {2010},
	pmid = {17504609},
	note = {arXiv: cond-mat/0307085
ISBN: 9781627480031},
	pages = {1--122}
}

@article{Janer2009,
	title = {Supporting {Soundscape} {Design} in {Virtual} {Environments} with {Content}-based {Audio} {Retrieval}},
	volume = {2},
	issn = {1941-8477},
	url = {https://journals.tdl.org/jvwr/index.php/jvwr/article/view/635},
	doi = {10.4101/jvwr.v2i3.635},
	abstract = {The computer-assisted design of soundscapes for virtual environments has received far less attention than the creation of graphical content. In this “think piece” we briefly introduce the principal characteristics of a framework under development that aims towards the creation of an automatic sonification of virtual worlds. As a starting point, the proposed system is based on an on-line collaborative sound repository that, together with content-based audio retrieval tools, assists the search of sounds to be associated with 3D models or scenes.},
	number = {3},
	urldate = {2018-11-19},
	journal = {Journal For Virtual Worlds Research},
	author = {Janer, Jordi},
	month = sep,
	year = {2009},
	keywords = {audio retrieval, based, content, freesound, soundscape, virtual worlds}
}

@article{Gygi2010,
	title = {Development of the {Database} for {Environmental} {Sound} {Research} and {Application} ({DESRA}): {Design}, {Functionality}, and {Retrieval} {Considerations}},
	volume = {2010},
	issn = {1687-4714},
	url = {http://asmp.eurasipjournals.com/content/2010/1/654914},
	doi = {10.1155/2010/654914},
	abstract = {Theoretical and applied environmental sounds research is gaining prominence but progress has been hampered by the lack of a comprehensive, high quality, accessible database of environmental sounds. An ongoing project to develop such a resource is described, which is based upon experimental evidence as to the way we listen to sounds in the world. The database will include a large number of sounds produced by different sound sources, with a thorough background for each sound file, including experimentally obtained perceptual data. In this way DESRA can contain a wide variety of acoustic, contextual, semantic, and behavioral information related to an individual sound. It will be accessible on the Internet and will be useful to researchers, engineers, sound designers, and musicians.},
	number = {1},
	urldate = {2018-11-19},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Gygi, Brian and Shafiro, Valeriy},
	month = jun,
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	keywords = {Acoustics, Engineering Acoustics, Image and Speech Processing, Mathematics in Music, Signal},
	pages = {1--12}
}

@misc{Roy1997,
	title = {Wearable {Audio} {Computing} : {A} {Survey} of {Interaction} {Techniques}},
	url = {https://www.semanticscholar.org/paper/Wearable-Audio-Computing-%3A-A-Survey-of-Interaction-Roy-Sawhney/421a9204eeb9b7b2a2d49a974cfd7531e6b8eac8},
	urldate = {2018-11-19},
	author = {Roy, Deb and Sawhney, Nitin Nick and Schmandt, Chris and Pentland, Alex},
	year = {1997}
}

@article{Fernstrom2001,
	title = {Sonic browsing: {An} auditory tool for multimedia asset management},
	url = {https://smartech.gatech.edu/handle/1853/50644},
	abstract = {Presented at the 7th International Conference on Auditory Display (ICAD), Espoo, Finland, July 29-August 1, 2001.},
	urldate = {2018-11-19},
	author = {Fernstrom, Mikael and Brazil, Eoin},
	year = {2001},
	note = {Publisher: Georgia Institute of Technology},
	keywords = {Auditory display, Proceedings, Sonic browsing}
}

@inproceedings{Roma2012,
	address = {New York, New York, USA},
	title = {Active learning of custom sound taxonomies in unstructured audio data},
	isbn = {978-1-4503-1329-2},
	url = {http://dl.acm.org/citation.cfm?doid=2324796.2324872},
	doi = {10.1145/2324796.2324872},
	urldate = {2018-11-19},
	booktitle = {Proceedings of the 2nd {ACM} {International} {Conference} on {Multimedia} {Retrieval} - {ICMR} '12},
	publisher = {ACM Press},
	author = {Roma, Gerard and Janer, Jordi and Herrera, Perfecto},
	year = {2012},
	keywords = {audio databases, content based retrieval, interactive search, interface},
	pages = {1}
}

@article{Campbell1997,
	title = {Speaker recognition: a tutorial},
	volume = {85},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/628714/},
	doi = {10.1109/5.628714},
	abstract = {A tutorial on the design and development of automatic speaker- recognition systems is presented. Automatic speaker recognition is the use of a machine to recognize a person from a spoken phrase. These systems can operate in two modes: to identify a particular person or to verify a person’s claimed identity. Speech processing and the basic components of automatic speaker- recognition systems are shown and design tradeoffs are discussed. Then, a new automatic speaker-recognition system is given. This recognizer performs with 98.9\% correct identification. Last, the performances of various systems are compared.},
	number = {9},
	journal = {Proceedings of the IEEE},
	author = {Campbell, J.P.},
	year = {1997},
	pmid = {16402617},
	note = {ISBN: 0018-9219},
	keywords = {Access control, Authentication, Biomedical measurements, Biomedical signal processing, Biomedical transducers, Biometric, Communication system security, Computer nehvork security, Computer security, Corpus, Data bases, Identification of persons, Public safety},
	pages = {1437--1462}
}

@inproceedings{Gemmeke2017,
	title = {Audio {Set}: {An} ontology and human-labeled dataset for audio events},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7952261/},
	doi = {10.1109/ICASSP.2017.7952261},
	abstract = {Audio event recognition, the human-like ability to identify and re-late sounds from audio, is a nascent problem in machine percep-tion. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets – principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the de-velopment of high-performance audio event recognizers.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
	month = mar,
	year = {2017},
	note = {ISSN: 15206149},
	keywords = {Audio event detection, audio databases, data collection, sound ontology},
	pages = {776--780}
}

@article{Specht1990,
	title = {Probabilistic neural networks},
	volume = {3},
	issn = {08936080},
	url = {https://www.sciencedirect.com/science/article/pii/089360809090049Q},
	doi = {10.1016/0893-6080(90)90049-Q},
	abstract = {By replacing the sigmoid activation function often used in neural networks with an exponential function, a probabilistic neural network (PNN) that can compute nonlinear decision boundaries which approach the Bayes optimal is formed. Alternate activation functions having similar properties are also discussed. A fourlayer neural network of the type proposed can map any input pattern to any number of classifications. The decision boundaries can be modified in real-time using new data as they become available, and can be implemented using artificial hardware “neurons” that operate entirely in parallel. Provision is also made for estimating the probability and reliability of a classification as well as making the decision. The technique offers a tremendous speed advantage for problems in which the incremental adaptation time of back propagation is a significant fraction of the total computation time. For one application, the PNN paradigm was 200,000 times faster than back-propagation.},
	number = {1},
	urldate = {2018-10-28},
	journal = {Neural Networks},
	author = {Specht, Donald F.},
	month = jan,
	year = {1990},
	note = {Publisher: Pergamon},
	pages = {109--118}
}

@article{Kilgarriff2000,
	title = {{WordNet}: {An} {Electronic} {Lexical} {Database}},
	volume = {76},
	issn = {00978507},
	url = {https://www.jstor.org/stable/417141?origin=crossref},
	doi = {10.2307/417141},
	abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
	number = {3},
	journal = {Language},
	author = {Kilgarriff, Adam and Fellbaum, Christiane},
	month = sep,
	year = {2000},
	pmid = {21561289},
	note = {arXiv: 1011.1669v3
ISBN: 026206197X},
	pages = {706}
}

@article{Hsu2008,
	title = {Tag normalization and prediction for effective social media retrieval},
	doi = {10.1109/WIIAT.2008.92},
	abstract = {In this paper, we propose a tag normalization algorithm to unify the users' annotations. Meanwhile, we explore some general phenomena in a social annotation system and propose a supervised tag prediction model to predict the stabilized tag set of a resource, with feedback of a small amount of user annotation records. The experiments show that a large potion of the stabilized tag set is predicted, and it is feasible to reduce the requirement of sufficient user annotations in the applications of social annotations. {\textbackslash}n},
	journal = {Proceedings - 2008 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2008},
	author = {Hsu, Ming Hung and Chen, Hsin Hsi},
	year = {2008},
	note = {ISBN: 9780769534961},
	keywords = {10, 4, analyzed the usage patterns, explored, golder and huberman, in, of websites, property to estimate the, similarity between two, social annotations for semantic, terms and the quality, web construction, wu et al},
	pages = {770--774}
}

@article{Lew2006,
	title = {Content-based multimedia information retrieval},
	volume = {2},
	issn = {15516857},
	url = {http://portal.acm.org/citation.cfm?doid=1126004.1126005},
	doi = {10.1145/1126004.1126005},
	abstract = {Extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. This survey reviews 100+ recent articles on content-based multimedia information retrieval and discusses their role in current research directions which include browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. Based on the current state of the art, we discuss the major challenges for the future.},
	number = {1},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Lew, Michael S and Sebe, Nicu and Djeraba, Chabane and Jain, Ramesh},
	month = feb,
	year = {2006},
	note = {ISBN: 1551-6857},
	pages = {1--19}
}

@article{Eronen2006,
	title = {Audio-based context recognition},
	volume = {14},
	issn = {1558-7916},
	url = {papers3://publication/uuid/1D35B777-4640-4436-A9B0-C44427743C2E},
	doi = {10.1109/TSA.2005.854103},
	number = {1},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Eronen, A.J. and Peltonen, V.T. and Tuomi, J.T. and Klapuri, A.P. and Fagerlund, Seppo and Sorsa, Timo and Lorho, Gaëtan and Huopaniemi, J.},
	month = jan,
	year = {2006},
	pages = {321--329}
}

@book{Karim2017,
	title = {Predictive {Analytics} with {TensorFlow}.},
	isbn = {978-1-78839-012-5},
	url = {https://ebookcentral-1proquest-1com-1ytvkzsj01a25.han.sub.uni-goettingen.de/lib/subgoettingen/detail.action?docID=5122925},
	abstract = {""Installing TensorFlow from source"" ""Cover ""; ""Copyright""; ""Credits""; ""About the Author""; ""Acknowledgments""; ""About the Reviewers""; ""www.PacktPub.com""; ""Customer Feedback""; ""Table of Contents""; ""Preface""; ""Chapter 1: Basic Python and Linear Algebra for Predictive Analytics ""; ""A basic introduction to predictive analytics""; ""Why predictive analytics?""; ""Working principles of a predictive model""; ""A bit of linear algebra""; ""Programming linear algebra""; ""Installing and getting started with Python""; ""Installing on Windows""; ""Installing Python on Linux"" ""Installing and upgrading PIP (or PIP3)""""Installing Python on Mac OS""; ""Installing packages in Python""; ""Getting started with Python""; ""Python data types""; ""Using strings in Python""; ""Using lists in Python""; ""Using tuples in Python""; ""Using dictionary in Python""; ""Using sets in Python""; ""Functions in Python""; ""Classes in Python""; ""Vectors, matrices, graphs, and tensors""; ""Vectors""; ""Matrices""; ""Matrix addition""; ""Matrix subtraction""; ""Finding the determinant of a matrix""; ""Finding the transpose of a matrix""; ""Solving simultaneous linear equations"" ""Eigenvalues and eigenvectors""""Span and linear independence""; ""Principal component analysis""; ""Singular value decomposition""; ""Data compression in a predictive model using SVD""; ""Predictive analytics tools in Python""; ""Summary""; ""Chapter 2: Statistics, Probability, and Information Theory for Predictive Modeling ""; ""Using statistics in predictive modeling""; ""Statistical models""; ""Parametric versus nonparametric model""; ""Population and sample""; ""Random sampling""; ""Expectation""; ""Central limit theorem""; ""Skewness and data distribution"" ""Standard deviation and variance""""Covariance and correlation""; ""Interquartile, range, and quartiles""; ""Hypothesis testing""; ""Chi-square tests""; ""Chi-square independence test""; ""Basic probability for predictive modeling""; ""Probability and the random variables""; ""Generating random numbers and setting the seed""; ""Probability distributions""; ""Marginal probability""; ""Conditional probability""; ""The chain rule of conditional probability""; ""Independence and conditional independence""; ""Bayes' rule""; ""Using information theory in predictive modeling""; ""Self-information"" ""Mutual information""""Entropy""; ""Shannon entropy""; ""Joint entropy""; ""Conditional entropy""; ""Information gain""; ""Using information theory""; ""Using information theory in Python""; ""Summary""; ""Chapter 3: From Data to Decisions â\#x80;\#x93; Getting Started with TensorFlow ""; ""Taking decisions based on data -- Titanic example""; ""Data value chain for making decisions""; ""From disaster to decision â\#x80;\#x93; Titanic survival example""; ""General overview of TensorFlow""; ""Installing and configuring TensorFlow""; ""Installing TensorFlow on Linux""; ""Installing Python and nVidia driver""},
	author = {Karim, Md. Rezaul.},
	year = {2017}
}

@article{Abadi2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	issn = {0270-6474},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	doi = {10.1038/nn.3331},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	journal = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16)},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = may,
	year = {2016},
	pmid = {16411492},
	note = {arXiv: 1605.08695
ISBN: 978-1-931971-33-1},
	pages = {265--284}
}

@article{Reynolds2000,
	title = {Speaker verification using adapted {Gaussian} mixture models},
	volume = {10},
	issn = {10512004},
	doi = {10.1006/dspr.1999.0361},
	abstract = {In this paper we describe the major elements of MIT Lincoln Laboratory's Gaussian mixture model (GMM)-based speaker verification system used successfully in several NIST Speaker Recognition Evaluations (SREs). The system is built around the likelihood ratio test for verification, using simple but effective GMMs for likelihood functions, a universal background model (UBM) for alternative speaker representation, and a form of Bayesian adaptation to derive speaker models from the UBM. The development and use of a handset detector and score normalization to greatly improve verification performance is also described and discussed. Finally, representative performance benchmarks and system behavior experiments on NIST SRE corpora are presented.},
	number = {1},
	journal = {Digital Signal Processing: A Review Journal},
	author = {Reynolds, Douglas A. and Quatieri, Thomas F. and Dunn, Robert B.},
	year = {2000},
	note = {ISBN: 1051-2004},
	keywords = {gaussian mixture models, handset normalization, likelihood, nist, ratio detector, speaker recognition, universal background model},
	pages = {19--41}
}

@article{Font2013,
	title = {Freesound technical demo},
	url = {http://dl.acm.org/citation.cfm?doid=2502081.2502245},
	doi = {10.1145/2502081.2502245},
	abstract = {Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.},
	urldate = {2018-09-23},
	journal = {Proceedings of the 21st ACM international conference on Multimedia - MM '13},
	author = {Font, Frederic and Roma, Gerard and Serra, Xavier},
	year = {2013},
	note = {Publisher: ACM Press
Place: New York, New York, USA
ISBN: 9781450324045},
	keywords = {audio clips, freesound, online databases, sound},
	pages = {411--412}
}

@inproceedings{Piczak2015,
	address = {New York, New York, USA},
	title = {{ESC}},
	isbn = {978-1-4503-3459-4},
	url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
	doi = {10.1145/2733373.2806390},
	urldate = {2018-09-23},
	booktitle = {Proceedings of the 23rd {ACM} international conference on {Multimedia} - {MM} '15},
	publisher = {ACM Press},
	author = {Piczak, Karol J.},
	year = {2015},
	pages = {1015--1018}
}

@article{Turnbull2008,
	title = {Semantic annotation and retrieval of music and sound effects},
	volume = {16},
	issn = {15587916},
	url = {http://ieeexplore.ieee.org/document/4432652/},
	doi = {10.1109/TASL.2007.913750},
	abstract = {We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint proba-bility of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our " query-by-text " system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.},
	number = {2},
	urldate = {2018-09-23},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
	month = feb,
	year = {2008},
	keywords = {Audio annotation and retrieval, Music information retrieval, Semantic music analysis},
	pages = {467--476}
}

@article{Slaney2002,
	title = {Semantic-audio retrieval},
	issn = {1520-6149},
	url = {http://ieeexplore.ieee.org/document/5745561/},
	doi = {10.1109/ICASSP.2002.5745561},
	abstract = {This paper describes a system for connecting sounds and words in linked multi-dimensional vector spaces. The acoustic space is represented using anchor models and partitioned using agglomerative clustering. The semantic space is modeled by a hierarchical multinomial clustering model. Nodes in one space are linked by probabilistic models to the other space. With these linked models, users retrieve sounds with natural language, and the system describes new sounds with words.},
	journal = {IEEE International Conference on Acoustics Speech and Signal Processing},
	author = {Slaney, Malcolm},
	year = {2002},
	note = {ISBN: 0-7803-7402-9},
	pages = {IV--4108--IV--4111}
}
@article{Turnbull2007,
	title = {Towards musical query-by-semantic-description using the cal500 data set},
	url = {http://portal.acm.org/citation.cfm?doid=1277741.1277817},
	doi = {10.1145/1277741.1277817},
	abstract = {Query-by-semantic-description (QBSD)is a natural paradigm for retrieving content from large databases of music. A major impediment to the development of good QBSD systems for music information retrieval has been the lack of a cleanly-labeled, publicly-available, heterogeneous data set of songs and associated annotations. We have collected the Computer Audition Lab 500-song (CAL500) data set by having humans listen to and annotate songs using a survey designed to capture 'semantic associations' between music and words. We adapt the supervised multi-class labeling (SML) model, which has shown good performance on the task of image retrieval, and use the CAL500 data to learn a model for music retrieval. The model parameters are estimated using the weighted mixture hierarchies expectation-maximization algorithm which has been specifically designed to handle real-valued semantic association between words and songs, rather than binary class labels. The output of the SML model, a vector of class-conditional probabilities, can be interpreted as a semantic multinomial distribution over a vocabulary. By also representing a semantic query as a query multinomial distribution, we can quickly rank order the songs in a database based on the Kullback-Leibler divergence between the query multinomial and each song's semantic multinomial. Qualitative and quantitative results demonstrate that our SML model can both annotate a novel song with meaningful words and retrieve relevant songs given a multi-word, text-based query.},
	number = {January},
	urldate = {2018-09-23},
	journal = {Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
	author = {Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
	year = {2007},
	note = {Publisher: ACM Press
Place: New York, New York, USA
ISBN: 9781595935977},
	keywords = {content-based music information retrieval, query-by-semantic-description, sification, supervised multi-class clas-},
	pages = {439--446}
}

@phdthesis{Vanderveer1980,
	address = {US},
	title = {Ecological acoustics: {Human} perception of environmental sounds.},
	school = {ProQuest Information \& Learning},
	author = {Vanderveer, Nancy J},
	year = {1980},
	note = {Volume: 40
Issue: 9-B
ISBN: 0419-4217(Print)},
	keywords = {*Auditory Perception, *Environment, *Memory, Recognition (Learning)}
}

@article{Gaver1993,
	title = {What in the {World} {Do} {We} {Hear}?: {An} {Ecological} {Approach} to {Auditory} {Event} {Perception}},
	volume = {5},
	issn = {15326969},
	url = {http://www.tandfonline.com/doi/abs/10.1207/s15326969eco0501_1},
	doi = {10.1207/s15326969eco0501_1},
	abstract = {Everyday listening is the experience of hearing events in the world rather than sounds per se. In this article, I take an ecological approach to everyday listening to overcome constraints on its study implied by more traditional approaches. In particular, I am concerned with deveolping a new framework for describing sound in terms of audible sources attributes. An examination of the coninuum of structured energy from events to audition suggests that sound conveys information about events at locations in an environment. Qualitative descriptions of the physics of sound-producing events into those involving vibrating solids, gasses, or liquids. Within each of these categories, basic-level events are defined by the simple interactions that can cause these materials to sound, whereas more complex events can be descrived in terms of temporal patterning, compound, or hybrid sources. The results of these investigations are used to creat a map of sound-producing events and their attributes useful in guiding furhter exploration.},
	number = {1},
	urldate = {2018-09-22},
	journal = {Ecological Psychology},
	author = {Gaver, William W.},
	month = mar,
	year = {1993},
	pmid = {768},
	note = {Publisher: Lawrence Erlbaum Associates, Inc.
ISBN: 1040-7413},
	pages = {1--29}
}

@article{Bardeli2009,
	title = {Similarity {Search} in {Animal} {Sound} {Databases}},
	volume = {11},
	issn = {1520-9210},
	url = {http://ieeexplore.ieee.org/document/4729673/},
	doi = {10.1109/TMM.2008.2008920},
	abstract = {In the past, similarity search for audio data has largely been focused on music. Recent digitization efforts in some of the larger animal sound archives bring other types of audio recordings into the focus of interest. Although recordings in animal sound archives are usually very well annotated by metadata, it is almost impossible to manually annotate all sounds made by animals in each recording. Complementary to classical text-based querying of databases that exploit available annotations, algorithms capable of automatically finding sections of recordings similar to a given query fragment provide a promising approach for content-based navigation. In our work, we present algorithms for feature extraction, as well as indexing and retrieval of animal sound recordings. Making use of a concept from image processing, the structure tensor, our feature extraction algorithm is adapted to the typical curve-like spectral features that are characteristic for many types of animal sounds. We propose a method for similarity search in animal sound databases which is obtained by adding a novel ranking scheme to an existing inverted file based approach for multimedia retrieval. Evaluation of our methods is based on recordings from the Animal Sound Archive, Berlin.},
	number = {1},
	journal = {IEEE Transactions on Multimedia},
	author = {Bardeli, R.},
	month = jan,
	year = {2009},
	note = {ISBN: 1520-9210},
	keywords = {Similarity search, animal sounds, feature extraction, indexing, retrieval, structure tensor},
	pages = {68--76}
}

@article{McLoughlin2015,
	title = {Robust {Sound} {Event} {Classification} {Using} {Deep} {Neural} {Networks}},
	volume = {23},
	issn = {2329-9290},
	url = {http://ieeexplore.ieee.org/document/7003973/},
	doi = {10.1109/TASLP.2015.2389618},
	abstract = {The automatic recognition of sound events by computers is an important aspect of emerging applications such as automated surveillance, machine hearing and auditory scene understanding. Recent advances in machine learning, as well as in computational models of the human auditory system, have contributed to advances in this increasingly popular research field. Robust sound event classification, the ability to recognise sounds under real-world noisy conditions, is an especially challenging task. Classification methods translated from the speech recognition domain, using features such as mel-frequency cepstral coefficients, have been shown to perform reasonably well for the sound event classification task, although spectrogram-based or auditory image analysis techniques reportedly achieve superior performance in noise. This paper outlines a sound event classification framework that compares auditory image front end features with spectrogram image-based front end features, using support vector machine and deep neural network classifiers. Performance is evaluated on a standard robust classification task in different levels of corrupting noise, and with several system enhancements, and shown to compare very well with current state-of-the-art classification techniques.},
	number = {3},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {McLoughlin, Ian and Zhang, Haomin and Xie, Zhipeng and Song, Yan and Xiao, Wei},
	month = mar,
	year = {2015},
	note = {ISBN: 9781467300469},
	keywords = {Auditory event detection, Auditory system, DNN, Feature extraction, Spectrogram, Speech, Speech processing, Support vector machines, Vectors, acoustic signal processing, auditory image front end feature, deep neural network, feature extraction, machine hearing, neural nets, signal classification, sound event classification, spectrogram image-based front end feature, support vector machine, support vector machines},
	pages = {540--552}
}

@inproceedings{Lane2015,
	address = {New York, New York, USA},
	title = {{DeepEar}},
	isbn = {978-1-4503-3574-4},
	url = {http://dl.acm.org/citation.cfm?doid=2750858.2804262},
	doi = {10.1145/2750858.2804262},
	abstract = {Microphones are remarkably powerful sensors of human be- havior and context. However, audio sensing is highly sus- ceptible to wild fluctuations in accuracy when used in di- verse acoustic environments (such as, bedrooms, vehicles, or cafes), that users encounter on a daily basis. Towards ad- dressing this challenge, we turn to the field of deep learn- ing; an area of machine learning that has radically changed related audio modeling domains like speech recognition. In this paper, we present DeepEar – the first mobile audio sens- ing framework built from coupled Deep Neural Networks (DNNs) that simultaneously perform common audio sens- ing tasks. We train DeepEar with a large-scale dataset in- cluding unlabeled data from 168 place visits. The resulting learned model, involving 2.3M parameters, enables DeepEar to significantly increase inference robustness to background noise beyond conventional approaches present in mobile de- vices. Finally, we show DeepEar is feasible for smartphones by building a cloud-free DSP-based prototype that runs con- tinuously, using only 6\% of the smartphone’s battery daily.},
	urldate = {2018-09-22},
	booktitle = {Proceedings of the 2015 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} - {UbiComp} '15},
	publisher = {ACM Press},
	author = {Lane, Nicholas D. and Georgiev, Petko and Qendro, Lorena},
	year = {2015},
	keywords = {audio sensing, deep learning, mobile sensing},
	pages = {283--294}
}

@article{Kim2012,
	title = {Latent acoustic topic models for unstructured audio classification},
	volume = {1},
	issn = {17441382},
	url = {http://www.journals.cambridge.org/abstract_S2048770312000078},
	doi = {10.1017/ATSIP.2012.7},
	abstract = {© The Authors, 2012.We propose the notion of latent acoustic topics to capture contextual information embedded within a collection of audio signals. The central idea is to learn a probability distribution over a set of latent topics of a given audio clip in an unsupervised manner, assuming that there exist latent acoustic topics and each audio clip can be described in terms of those latent acoustic topics. In this regard, we use the latent Dirichlet allocation (LDA) to implement the acoustic topic models over elemental acoustic units, referred as acoustic words, and perform text-like audio signal processing. Experiments on audio tag classification with the BBC sound effects library demonstrate the usefulness of the proposed latent audio context modeling schemes. In particular, the proposed method is shown to be superior to other latent structure analysis methods, such as latent semantic analysis and probabilistic latent semantic analysis. We also demonstrate that topic models can be used as complementary features to content-based features and offer about 9\% relative improvement in audio classification when combined with the traditional Gaussian mixture model (GMM)-Support Vector Machine (SVM) technique.},
	number = {2},
	urldate = {2018-09-22},
	journal = {Journal of Institutional Economics},
	author = {Kim, Samuel and Georgiou, Panayiotis and Narayanan, Shrikanth},
	month = dec,
	year = {2012},
	note = {Publisher: Cambridge University Press
ISBN: 2048770312},
	keywords = {Acoustic topic models, Audio information retrieval, Latent topic models, Unstructured Audio, text-like audio signal processing},
	pages = {e6}
}

@inproceedings{Chechik2008,
	address = {New York, New York, USA},
	title = {Large-scale content-based audio retrieval from text queries},
	isbn = {978-1-60558-312-9},
	url = {http://portal.acm.org/citation.cfm?doid=1460096.1460115},
	doi = {10.1145/1460096.1460115},
	abstract = {In content-based audio retrieval, the goal is to find sound recordings (audio documents) based on their acoustic features. This content-based approach differs from retrieval approaches that index media files using metadata such as file names and user tags. In this paper, we propose a machine learning approach for retrieving sounds that is novel in that it (1) uses free-form text queries rather sound sample based queries, (2) searches by audio content rather than via textual meta data, and (3) can scale to very large number of audio documents and very rich query vocabulary. We handle generic sounds, including a wide variety of sound effects, animal vocalizations and natural scenes. We test a scalable approach based on a passive-aggressive model for image retrieval (PAMIR), and compare it to two state-of-the-art approaches; Gaussian mixture models (GMM) and support vector machines (SVM). We test our approach on two large real-world datasets: a collection of short sound effects, and a noisier and larger collection of user-contributed user-labeled recordings (25K files, 2000 terms vocabulary). We find that all three methods achieved very good retrieval performance. For instance, a positive document is retrieved in the first position of the ranking more than half the time, and on average there are more than 4 positive documents in the first 10 retrieved, for both datasets. PAMIR completed both training and retrieval of all data in less than 6 hours for both datasets, on a single machine. It was one to three orders of magnitude faster than the competing approaches. This approach should therefore scale to much larger datasets in the future.},
	booktitle = {Proceeding of the 1st {ACM} international conference on {Multimedia} information retrieval - {MIR} '08},
	publisher = {ACM Press},
	author = {Chechik, Gal and Ie, Eugene and Rehn, Martin and Bengio, Samy and Lyon, Dick},
	year = {2008},
	pages = {105}
}

@article{Pons2017a,
	title = {Assessing machine learning classifiers for the detection of animals’ behavior using depth-based tracking},
	volume = {86},
	issn = {09574174},
	doi = {10.1016/j.eswa.2017.05.063},
	abstract = {There is growing interest in the automatic detection of animals’ behaviors and body postures within the field of Animal Computer Interaction, and the benefits this could bring to animal welfare, enabling remote communication, welfare assessment, detection of behavioral patterns, interactive and adaptive systems, etc. Most of the works on animals’ behavior recognition rely on wearable sensors to gather information about the animals’ postures and movements, which are then processed using machine learning techniques. However, non-wearable mechanisms such as depth-based tracking could also make use of machine learning techniques and classifiers for the automatic detection of animals’ behavior. These systems also offer the advantage of working in set-ups in which wearable devices would be difficult to use. This paper presents a depth-based tracking system for the automatic detection of animals’ postures and body parts, as well as an exhaustive evaluation on the performance of several classification algorithms based on both a supervised and a knowledge-based approach. The evaluation of the depth-based tracking system and the different classifiers shows that the system proposed is promising for advancing the research on animals’ behavior recognition within and outside the field of Animal Computer Interaction.},
	journal = {Expert Systems with Applications},
	author = {Pons, Patricia and Jaen, Javier and Catala, Alejandro},
	year = {2017},
	keywords = {Animal Computer Interaction, Classification algorithms, Depth-based tracking, Intelligent system, Tracking system},
	pages = {235--246}
}

@inproceedings{Brugarolas2016,
	title = {Towards a wearable system for continuous monitoring of sniffing and panting in dogs},
	isbn = {978-1-5090-3087-3},
	doi = {10.1109/BSN.2016.7516276},
	abstract = {© 2016 IEEE.Although numerous advances have been made in instrumental odor detection systems, these still cannot match the efficient sampling, odor discrimination, agile mobility and the olfactory acuity of odor detection dogs. A limiting step in using dogs to detect odors is the subjectivity of the translation of odor information processed by the dog to its handler. We present our preliminary efforts towards a wireless wearable system for continuous auscultation of respiratory behavior by recording internal sounds at the neck and chest by means of a commercially available electronic stethoscope to provide objective decision support for handlers. We have identified discrete features of sniffing and panting in the time domain and utilize event duration, event rate, event mean energy, and the number of consecutive events in a row to build a decision tree classifier. Since feature extraction requires segmentation of individual sniffing and panting events, we developed an adaptive method using short-time energy contour and an adaptive threshold. The performance of the system was evaluated on recordings from a Greyhound and a Labrador Retriever and achieved high classification accuracies.},
	booktitle = {{BSN} 2016 - 13th {Annual} {Body} {Sensor} {Networks} {Conference}},
	author = {Brugarolas, R. and Agcayazi, T. and Yuschak, S. and Roberts, D. L. and Sherman, B. L. and Bozkurt, A.},
	year = {2016},
	pages = {292--295}
}

@article{Paci2016,
	title = {Towards a wearer-centred framework for animal biotelemetry},
	number = {May},
	author = {Paci, Patrizia and Mancini, Clara and Price, Blaine A},
	year = {2016},
	pages = {25--27}
}

@book{Garton2001,
	title = {Radio {Tracking} and {Animal} {Populations}},
	isbn = {978-0-12-497781-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780124977815500037},
	abstract = {Design studies that use radiotelemetry require careful consideration of the goals of a project and the resources available to meet those goals. Success in meeting the research goals depends on thoughtful planning of field methods and ancillary data collection, selection of telemetry equipment appropriate to the study animal and budget, careful execution of the field protocols, and creative analysis of the data. Some of the most important design factors include consideration of the study's purpose; degree of experimental manipulation, controls, and replication; selection of an efficient yet unbiased sampling scheme; definition of the sample unit; calculation of sample size requirements; identification and removal of sources of bias; and clear specification of biological significance. This chapter emphasizes the need to integrate univariate metrics of animal choice, such as estimates of home range size and resource selection, with metrics for the demographic consequences of these choices, all of which can be generated from radiotelemetry. Radiotelemetry is an essential tool in modern studies of movement, migration, and dispersal of most vertebrates. Its use has dramatically increased the amount and detail of information available for estimating movements of larger animals, and provides extremely valuable additions to information from studies that use tagging, banding (ringing), and other forms of marking for smaller animals. In addition, radiotelemetry studies are an important complement to recent approaches that use genetic markers.},
	urldate = {2018-07-03},
	publisher = {Academic Press},
	author = {Garton, Edward O. and Wisdom, Michael J. and Leban, Frederick A. and Johnson, Bruce K.},
	month = jan,
	year = {2001},
	doi = {10.1016/B978-012497781-5/50003-7},
	note = {Publication Title: Radio Tracking and Animal Populations}
}

@article{Pongracz2017,
	title = {Do you see what {I} see? {The} difference between dog and human visual perception may affect the outcome of experiments},
	volume = {140},
	issn = {18728308},
	doi = {10.1016/j.beproc.2017.04.002},
	abstract = {The visual sense of dogs is in many aspects different than that of humans. Unfortunately, authors do not explicitly take into consideration dog-human differences in visual perception when designing their experiments. With an image manipulation program we altered stationary images, according to the present knowledge about dog-vision. Besides the effect of dogs’ dichromatic vision, the software shows the effect of the lower visual acuity and brightness discrimination, too. Fifty adult humans were tested with pictures showing a female experimenter pointing, gazing or glancing to the left or right side. Half of the pictures were shown after they were altered to a setting that approximated dog vision. Participants had difficulty to find out the direction of glancing when the pictures were in dog-vision mode. Glances in dog-vision setting were followed less correctly and with a slower response time than other cues. Our results are the first that show the visual performance of humans under circumstances that model how dogs’ weaker vision would affect their responses in an ethological experiment. We urge researchers to take into consideration the differences between perceptual abilities of dogs and humans, by developing visual stimuli that fit more appropriately to dogs’ visual capabilities.},
	number = {March},
	journal = {Behavioural Processes},
	author = {Pongrácz, Péter and Ujvári, Vera and Faragó, Tamás and Miklósi, Ádám and Péter, András},
	year = {2017},
	note = {ISBN: 0201503311048},
	keywords = {Dog, Ethology, Human, Visual perception},
	pages = {53--60}
}

@article{BA¡lint2016,
	title = {Threat-level-dependent manipulation of signaled body size: dog growls’ indexical cues depend on the different levels of potential danger},
	volume = {19},
	issn = {14359448},
	doi = {10.1007/s10071-016-1019-9},
	abstract = {Body size is an important feature that affects fighting ability; however, size-related parameters of agonistic vocalizations are difficult to manipulate because of anatomical constraints within the vocal production system. Rare examples of acoustic size modulation are due to specific features that enable the sender to steadily communicate exaggerated body size. However, one could argue that it would be more adaptive if senders could adjust their signaling behavior to the fighting potential of their actual opponent. So far there has been no experimental evidence for this possibility. We tested this hypothesis by exposing family dogs (Canis familiaris) to humans with potentially different fighting ability. In a within-subject experiment, 64 dogs of various breeds consecutively faced two threateningly approaching humans, either two men or two women of different stature, or a man and a woman of similar or different stature. We found that the dogs’ vocal responses were affected by the gender of the threatening stranger and the dog owner’s gender. Dogs with a female owner, or those dogs which came from a household where both genders were present, reacted with growls of lower values of the Pitch–Formant component (including deeper fundamental frequency and lower formant dispersion) to threatening men. Our results are the first to show that non-human animals react with dynamic alteration of acoustic parameters related to their individual indexical features (body size), depending on the level of threat in an agonistic encounter.},
	number = {6},
	journal = {Animal Cognition},
	author = {BÃ¡lint, Anna and FaragÃ³, TamÃ¡s and MiklÃ³si, ÃdÃ¡m and PongrÃ¡cz, PÃ©ter},
	year = {2016},
	pmid = {28077769},
	note = {ISBN: 1007101610199},
	keywords = {Communication, Dog growl, Indexical information, Manipulation, Threat},
	pages = {1115--1131}
}

@article{Filippi2016,
	title = {Emotional and interactional prosody across animal communication systems: {A} comparative approach to the emergence of language},
	volume = {7},
	issn = {16641078},
	doi = {10.3389/fpsyg.2016.01393},
	abstract = {Across a wide range of animal taxa, prosodic modulation of the voice can express emotional information and is used to coordinate vocal interactions between multiple individuals. Within a comparative approach to animal communication systems, I hypothesize that the ability for emotional and interactional prosody (EIP) paved the way for the evolution of linguistic prosody - and perhaps also of music, continuing to play a vital role in the acquisition of language. In support of this hypothesis, I review three research fields: (i) empirical studies on the adaptive value of EIP in non-human primates, mammals, songbirds, anurans, and insects; (ii) the beneficial effects of EIP in scaffolding language learning and social development in human infants; (iii) the cognitive relationship between linguistic prosody and the ability for music, which has often been identified as the evolutionary precursor of language.},
	number = {SEP},
	journal = {Frontiers in Psychology},
	author = {Filippi, Piera},
	year = {2016},
	pmid = {27733835},
	keywords = {Arousal, Entrainment, Infant-directed speech, Interaction, Language evolution, Musical protolanguage, Prosody, Turn-taking}
}

@article{Larranaga2014,
	title = {Comparing supervised learning methods for classifying sex, age, context and individual {Mudi} dogs from barking},
	volume = {18},
	issn = {14359448},
	url = {http://link.springer.com/10.1007/s10071-014-0811-7},
	doi = {10.1007/s10071-014-0811-7},
	abstract = {Barking is perhaps the most characteristic form of vocalization in dogs; however, very little is known about its role in the intraspecific communication of this species. Besides the obvious need for ethological research, both in the field and in the laboratory, the possible information content of barks can also be explored by computerized acoustic analyses. This study compares four different supervised learning methods (naive Bayes, classification trees, [Formula: see text]-nearest neighbors and logistic regression) combined with three strategies for selecting variables (all variables, filter and wrapper feature subset selections) to classify Mudi dogs by sex, age, context and individual from their barks. The classification accuracy of the models obtained was estimated by means of [Formula: see text]-fold cross-validation. Percentages of correct classifications were 85.13 \% for determining sex, 80.25 \% for predicting age (recodified as young, adult and old), 55.50 \% for classifying contexts (seven situations) and 67.63 \% for recognizing individuals (8 dogs), so the results are encouraging. The best-performing method was [Formula: see text]-nearest neighbors following a wrapper feature selection approach. The results for classifying contexts and recognizing individual dogs were better with this method than they were for other approaches reported in the specialized literature. This is the first time that the sex and age of domestic dogs have been predicted with the help of sound analysis. This study shows that dog barks carry ample information regarding the caller's indexical features. Our computerized analysis provides indirect proof that barks may serve as an important source of information for dogs as well.},
	number = {2},
	urldate = {2018-04-27},
	journal = {Animal Cognition},
	author = {Larrañaga, Ana and Bielza, Concha and Pongrácz, Péter and Faragó, Tamás and Bálint, Anna and Larrañaga, Pedro},
	month = mar,
	year = {2014},
	pmid = {25308549},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 1435-9448},
	keywords = {Acoustic communication, Feature subset selection, K-fold cross-validation, Machine learning, Mudi dog barks, Supervised classification},
	pages = {405--421}
}

@article{Farago2017,
	title = {Dog growls express various contextual and affective content for human listeners},
	volume = {4},
	issn = {2054-5703},
	url = {http://rsos.royalsocietypublishing.org/lookup/doi/10.1098/rsos.170134},
	doi = {10.1098/rsos.170134},
	abstract = {© 2017 The Authors.Vocal expressions of emotions follow simple rules to encode the inner state of the caller into acoustic parameters, not just within species, but also in cross-species communication. Humans use these structural rules to attribute emotions to dog vocalizations, especially to barks, which match with their contexts. In contrast, humans were found to be unable to differentiate between playful and threatening growls, probably because single growls’ aggression level was assessed based on acoustic size cues. To resolve this contradiction, we played back natural growl bouts from three social contexts (food guarding, threatening and playing) to humans, who had to rate the emotional load and guess the context of the playbacks. Listeners attributed emotions to growls according to their social contexts. Within threatening and playful contexts, bouts with shorter, slower pulsing growls and showing smaller apparent body size were rated to be less aggressive and fearful, but more playful and happy. Participants associated the correct contexts with the growls above chance. Moreover, women and participants experienced with dogs scored higher in this task. Our results indicate that dogs may communicate honestly their size and inner state in a serious contest situation, while manipulatively in more uncertain defensive and playful contexts.},
	number = {5},
	urldate = {2018-04-27},
	journal = {Royal Society Open Science},
	author = {Faragó, T. and Takács, N. and Miklósi, Á. and Pongrácz, P.},
	month = may,
	year = {2017},
	pmid = {28573021},
	note = {Publisher: The Royal Society
ISBN: 0000000159},
	pages = {170134}
}

@book{Dietterich,
	title = {Introduction to {Machine} {Learning} {Second} {Edition} {Adaptive} {Computation} and {Machine} {Learning}},
	isbn = {978-0-262-19398-6},
	author = {Dietterich, Thomas and Bishop, Christopher and Heckerman, David and Jordan, Michael and Kearns, Michael}
}

@article{Minnen2006,
	title = {Performance {Metrics} and {Evaluation} {Issues} for {Continuous} {Activity} {Recognition}},
	issn = {21576904},
	doi = {10.1145/1889681.1889687},
	abstract = {In this paper we examine several factors that influ- ence the evaluation of multi-class, continuous activity recognition. Currently, there is no standard metric for evaluating and com- paring such systems, although many possible error formulations and performance metrics could be adapted from other domains. In order to make progress toward a standard metric appropriate for evaluating activity recognition, we outline the sources of errors in such systems, present different methods for detecting and labeling these errors, and compare existing metrics with a more nuanced performance visualization. We conclude with a discussion concerning the interpretation of the visualization for comparing recognition systems in different domains.},
	journal = {Proc. Int. Workshop on Performance Metrics for Intelligent Systems},
	author = {Minnen, David and Westeyn, Tracy L and Starner, Thad and Ward, Jamie a and Lukowicz, Paul},
	year = {2006},
	pages = {141--148}
}

@article{Horowitz2013,
	title = {Smelling more or less: {Investigating} the olfactory experience of the domestic dog},
	volume = {44},
	issn = {00239690},
	url = {https://www.sciencedirect.com/science/article/pii/S0023969013000234},
	doi = {10.1016/j.lmot.2013.02.002},
	abstract = {The performance of tracking dogs and drug-, disease-, and explosives-detection dogs is a testament to trained dogs' olfactory acuity. The olfactory experience of an untrained dog, by contrast, has not been well documented. In the current research we begin to remedy that by testing untrained pet dogs' olfactory perception of quantity. While previous research found that dogs could discriminate visible quantities of more or less food (Prato-Previde, Marshall-Pescini, \& Valsecchi, 2008), our results find that, by contrast, companion dogs do not reliably discriminate quantities when the food can be smelled but not seen. Sixty-one percent of dogs (39 of 64), given a choice between closed plates with one and five morsels of food, approached plates with the larger quantity: not significantly more than approached plates with the lesser quantity (binomial, p = .169). We did find that during dogs' initial investigation of both food amounts, subjects gave more attention to the plate containing the larger quantity (binomial, p{\textless} 0.001). In a second condition, we replicated, with closed plates, Prato-Previde et al.'s (2008) finding that owner interest in a plate holding a lesser quantity of food reliably leads dogs to approach that plate (binomial, p{\textless} 0.001). Though research has demonstrated dogs' preference for a larger amount of food (Ward \& Smuts, 2007), in a third condition testing the effect of adding a strong odor to a visibly larger food quantity, we found that the addition of odor often reversed that preference (44/69 dogs; p{\textless} .03). Finally, we consider the methodological implications of this work on future dog cognition studies. © 2013 Elsevier Inc.},
	number = {4},
	urldate = {2018-03-05},
	journal = {Learning and Motivation},
	author = {Horowitz, Alexandra and Hecht, Julie and Dedrick, Alexandra},
	month = nov,
	year = {2013},
	note = {Publisher: Academic Press
ISBN: 00239690 (ISSN)},
	pages = {207--217}
}

@article{Gogoleva2008,
	title = {To bark or not to bark: {Vocalizations} by red foxes selected for tameness or aggressiveness toward humans},
	volume = {18},
	issn = {21650586},
	doi = {10.1080/09524622.2008.9753595},
	abstract = {In this study we classify call structures and compare vocalizations toward humans by captive red foxes Vulpes vulpes, artificially selected for behaviour: 25 domesticated, or "Tame" animals, selected for tameness toward people, 25 "Aggressive" animals, selected for aggression toward people, and 25 "Unselected" control foxes, representing the "wild" model of vocal behaviour. In total, 12,964 calls were classified visually from spectrograms into five voiced (tonal) (whine, moo, cackle, growl and bark), and three unvoiced, or noisy (pant, snort and cough) call types. The classification results were verified with discriminant function analysis (DFA) and randomization. We found that the Aggressive and Unselected foxes produced the same call type sets toward humans, whereas the Tame foxes used distinctive vocalizations toward humans. The Tame and Aggressive foxes had significantly higher percentages of time spent vocalizing than the Unselected, in support of Cohen \& Fox (1976) hypothesis that domestication relaxes the selection pressure for silence, still acting in wild canids. Unlike in dogs, the "domesticated" Tame foxes did not show hypertrophied barking toward humans, using instead the cackle and pant. We conclude that the use of a certain call type for communication between humans and canids is species-specific, and not is the direct effect of domestication per se. ¬© 2008 AB Academic Publishers.},
	number = {2},
	journal = {Bioacoustics},
	author = {Gogoleva, S. S. and Volodin, J. A. and Volodina, E. V. and Trut, L. N.},
	year = {2008},
	pmid = {46},
	note = {ISBN: 0952-4622},
	keywords = {Articulation, Canidae, Domestication, Nonlinear phenomena, Red fox, Vocal communication, Vocalization, Vulpes vulpes},
	pages = {99--132}
}

@article{Gogoleva2011,
	title = {Explosive vocal activity for attracting human attention is related to domestication in silver fox},
	volume = {86},
	issn = {0376-6357},
	url = {https://www.sciencedirect.com/science/article/pii/S0376635710002949},
	doi = {10.1016/J.BEPROC.2010.12.001},
	abstract = {Domestication affects behavioral and vocal responses, involved in communication with humans; in particular, those that attract human attention. In this study, we found that silver foxes of Tame strain, experimentally domesticated for a few tenses of generation, displayed bursts of vocal activity during the first minute after appearance of an unfamiliar human, that faded quickly during the remaining time of the test, when the experimenter stayed passively before the cage. Distinctively, foxes of Aggressive strain, artificially selected for tenses of generation for aggressive behavior toward humans, and the control group of Unselected for behavior silver foxes kept steady levels of vocal activity for the duration of the tests. We found also that Aggressive foxes vocalized for a larger proportion of time than Unselected foxes for all 5min of the test. We discuss the obtained data in relation to proposal effects of domestication on mechanisms directed to involving people into human–animal interactions and structural similarity between human laughter and vocalization of Tame foxes.},
	number = {2},
	urldate = {2018-02-18},
	journal = {Behavioural Processes},
	author = {Gogoleva, Svetlana S. and Volodin, Ilya A. and Volodina, Elena V. and Kharlamova, Anastasia V. and Trut, Lyudmila N.},
	month = feb,
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {216--221}
}

@article{Izso2000,
	title = {Heart period variability as mental effort monitor in {Human} {Computer} {Interaction}},
	volume = {19},
	issn = {0144929X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01449290050086408},
	doi = {10.1080/01449290050086408},
	abstract = {Elementary steps of H uman Computer Interaction (HCI), like users' mental actions followed by a series of keystrokes and mouse-clicks, are the basic components of using information technological systems. This is why examination methods capable of assessing users' actual mental effort corresponding to these elementary steps during HCI in a scientifically sound way have great importance. It is known that under certain circumstances, Heart Period Variability (HPV) could be a measure of actual mental effort. This paper gives a short overview of applications of HPV in ergonomics in general and, based on empirical evidence intends to prove that this methodology, after a careful adaptation, could be powerful technique for monitoring mental effort in HCI. The paper outlines the main components of the INTERFACE testing workstation and the related methodology for investigatingamong others-users mental effort. A detailed application example is also provided.},
	number = {4},
	urldate = {2018-02-17},
	journal = {Behaviour and Information Technology},
	author = {Izsó, Lajos and Láng, Eszter},
	month = jan,
	year = {2000},
	note = {Publisher: Taylor \& Francis Group
ISBN: 0144-929X},
	pages = {297--306}
}

@article{Albuquerque2016,
	title = {Dogs recognize dog and human emotions.},
	volume = {12},
	issn = {1744-957X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/26763220},
	doi = {10.1098/rsbl.2015.0883},
	abstract = {The perception of emotional expressions allows animals to evaluate the social intentions and motivations of each other. This usually takes place within species; however, in the case of domestic dogs, it might be advantageous to recognize the emotions of humans as well as other dogs. In this sense, the combination of visual and auditory cues to categorize others' emotions facilitates the information processing and indicates high-level cognitive representations. Using a cross-modal preferential looking paradigm, we presented dogs with either human or dog faces with different emotional valences (happy/playful versus angry/aggressive) paired with a single vocalization from the same individual with either a positive or negative valence or Brownian noise. Dogs looked significantly longer at the face whose expression was congruent to the valence of vocalization, for both conspecifics and heterospecifics, an ability previously known only in humans. These results demonstrate that dogs can extract and integrate bimodal sensory emotional information, and discriminate between positive and negative emotions from both humans and dogs.},
	number = {1},
	urldate = {2018-02-16},
	journal = {Biology letters},
	author = {Albuquerque, Natalia and Guo, Kun and Wilkinson, Anna and Savalli, Carine and Otta, Emma and Mills, Daniel},
	month = jan,
	year = {2016},
	pmid = {26763220},
	note = {Publisher: The Royal Society},
	keywords = {Canis familiaris, cross-modal sensory integration, emotion recognition, social cognition},
	pages = {20150883}
}

@article{Rosas,
	title = {C {Clever} {Hans}},
	doi = {10.1007/978-3-319-47829-6_1283-1},
	urldate = {2018-02-13},
	author = {Rosas, Juan M}
}

@article{Molnar2008,
	title = {Classification of dog barks: a machine learning approach},
	volume = {11},
	issn = {1435-9448},
	url = {http://link.springer.com/10.1007/s10071-007-0129-9},
	doi = {10.1007/s10071-007-0129-9},
	number = {3},
	urldate = {2018-02-08},
	journal = {Animal Cognition},
	author = {Molnár, Csaba and Kaplan, Frédéric and Roy, Pierre and Pachet, François and Pongrácz, Péter and Dóka, Antal and Miklósi, Ádám},
	month = jul,
	year = {2008},
	note = {Publisher: Springer-Verlag},
	pages = {389--400}
}

@article{Molnar2009,
	title = {Dogs discriminate between barks: {The} effect of context and identity of the caller},
	volume = {82},
	issn = {0376-6357},
	url = {https://www.sciencedirect.com/science/article/pii/S0376635709001557},
	doi = {10.1016/J.BEPROC.2009.06.011},
	abstract = {In the present study we explored whether dogs (Canis familiaris) are able to discriminate between conspecific barks emitted in different contexts recorded either from the same or different individuals. Playback experiments were conducted with dogs using barks as stimuli in a habituation–dishabituation paradigm. Barks were recorded in two contexts (stranger at the fence and when the dog was left alone) from different individuals. We found that dogs distinguished between barks emitted in these two contexts and were also able to discriminate between different individuals which were barking in the same context. These findings suggest that dog bark may carry context- and individual-specific information for the conspecifics.},
	number = {2},
	urldate = {2018-02-07},
	journal = {Behavioural Processes},
	author = {Molnár, Csaba and Pongrácz, Péter and Faragó, Tamás and Dóka, Antal and Miklósi, Ádám},
	month = oct,
	year = {2009},
	note = {Publisher: Elsevier},
	pages = {198--201}
}

@article{Maros2008,
	title = {Dogs can discriminate barks from different situations},
	volume = {114},
	issn = {0168-1591},
	url = {https://www.sciencedirect.com/science/article/pii/S0168159108000427?_rdoc=1&_fmt=high&_origin=gateway&_docanchor=&md5=b8429449ccfc9c30159a5f9aeaa92ffb&ccp=y},
	doi = {10.1016/J.APPLANIM.2008.01.022},
	abstract = {We investigated if dogs can discriminate barks of another individual recorded in two markedly different situations: (a) when a stranger entered the property where the dog lived, and (b) when the dog was tethered to a tree and left alone. We used a habituation–dishabituation paradigm for testing discriminatory abilities. Three 25-s long samples of “stranger” bark were followed by a single “alone” bark sample. As a control, we used two types of mechanical noise (an electric drill and a refrigerator). Dogs (n=14) were equipped with a portable heart rate monitor which recorded the data during the whole experiment. Upon hearing the first barking sound, the heart rate of the dogs increased significantly, followed by a habituation when the same barks were played back the second and third time. The fourth, different bark caused dishabituation of the heart rate. This suggests that heart rate can be a sensitive indicator of changes in attentiveness. The dogs did not show any significant evidence of dishabituation to the Control condition of the mechanical noises. Our experiment showed that dogs can perceive the difference between barks originating from different situations, thus barking is perhaps a communicative tool not only for dogs to humans, but for dogs to dogs as well.},
	number = {1-2},
	urldate = {2018-02-07},
	journal = {Applied Animal Behaviour Science},
	author = {Maros, Katalin and Pongrácz, Péter and Bárdos, György and Molnár, Csaba and Faragó, Tamás and Miklósi, Ádám},
	month = nov,
	year = {2008},
	note = {Publisher: Elsevier},
	pages = {159--167}
}

@article{Peter2014,
	title = {More than noise?—{Field} investigations of intraspecific acoustic communication in dogs ({Canis} familiaris)},
	volume = {159},
	issn = {0168-1591},
	url = {https://www.sciencedirect.com/science/article/pii/S0168159114002093},
	doi = {10.1016/J.APPLANIM.2014.08.003},
	abstract = {Besides being a widely investigated behavioural phenomenon, barks of dogs often represent a factor of nuisance for people. Although some argue that dog barking has no or only minimal communicative function, it was shown recently that these acoustic signals carry various information that humans can decipher. However, apart from a few laboratory studies, until now no targeted research has been done about the communicative role of barks in the intraspecific domain. In this field experiment companion dogs were tested with bark playbacks at home, in a suburban environment. From a hidden sound system, placed near to the gate outside of the property, each subject was exposed to pre-recorded barks of an unfamiliar and a familiar dog. Barks for the playbacks were recorded in two different contexts: when the dog was either left alone or when it was barking at a stranger at the fence. We found differences in the behaviour of dogs depending on both the familiarity and context of the playback barks. The position of the dogs (near the house or near the gate) was mainly influenced by the context of the barks (p=0.011), in a significant interaction with the familiarity of the barking dog (p=0.020). Subjects stayed at the gate (nearest to the source of the sound) the longest when they heard an unfamiliar dog barking at a stranger (padj=0.012). Meanwhile they stayed at the house mostly during the barks of a lonely unfamiliar dog (padj=0.001). Dogs oriented more towards the house (where the familiar dog stayed during the experiment) when they heard the familiar dog's barking (p=0.019). Subjects barked more often when they heard the ‘stranger’ barks, independently of the familiarity of the caller (p=0.035). As a conclusion, dogs seemingly distinguished among the callers based on familiarity and between the contexts of the barks. This is the first study on companion dogs in their natural environment that found evidence that dogs are able to extract detailed information from the barks. The relevance of our findings for the management of excessive bark is discussed.},
	urldate = {2018-02-05},
	journal = {Applied Animal Behaviour Science},
	author = {Péter, Pongrácz and Éva, Szabó and Anna, Kis and András, Péter and Ádám, Miklósi},
	month = oct,
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {62--68}
}

@article{Pongracz2005,
	title = {Human listeners are able to classify dog ({Canis} familiaris) barks recorded in different situations},
	volume = {119},
	issn = {07357036},
	doi = {10.1037/0735-7036.119.2.136},
	abstract = {The authors investigated whether human listeners could categorize played-back dog (Canis familiaris) barks recorded in various situations and associate them with emotional ratings. Prerecorded barks of a Hungarian herding dog breed (Mudi) provided the sample. Human listeners were asked to rate emotionality of the vocalization and to categorize the situations on the basis of alternative situations provided on a questionnaire. The authors found almost no effect of previous experience with the given dog breed or of owning a dog. Listeners were able to categorize bark situations high above chance level. Emotionality ratings for particular bark samples correlated with peak and fundamental frequency and interbark intervals. The authors did not find a significant effect of tonality (harmonic-to-noise ratio) on either the emotionality rating or situation categorization of the human listeners. Humans' ability to recognize meaning suggests that barks could serve as an effective means of communication between dog and human.},
	number = {2},
	journal = {Journal of Comparative Psychology},
	author = {Pongrácz, Péter and Molnár, Csaba and Miklósi, Ádám and Csányi, Vilmos},
	year = {2005},
	pmid = {15982157},
	note = {ISBN: 0735-7036},
	pages = {136--144}
}

@article{Pongracz2010,
	title = {Barking in family dogs: {An} ethological approach},
	volume = {183},
	issn = {1090-0233},
	url = {https://www.sciencedirect.com/science/article/pii/S1090023308004437},
	doi = {10.1016/J.TVJL.2008.12.010},
	abstract = {Although it is one of the most conspicuous features of dog behaviour, barking has received little attention from ethologists or from an applied perspective. In this review, an ethological look is taken at the communicative aspect of dog barking. Emerging new research has indicated that in the repertoire of dog vocalisations barking has unique features in showing wide ranges of acoustic parameters, such as frequency, tonality and rhythmicity. Barking has been shown to be context dependent, and provides information for humans about the inner state of the dog although there are few indications that barking is used for intra-species communication. It is assumed that dog barking emerged through selective processes in which human preferences for certain acoustic aspects of the vocalisation may have been paramount. A more experiment-oriented approach is required for the study of dog vocalisation that could shed light on the possible communicative function of these acoustic signals.},
	number = {2},
	urldate = {2018-02-05},
	journal = {The Veterinary Journal},
	author = {Pongrácz, Péter and Molnár, Csaba and Miklósi, Ádám},
	month = feb,
	year = {2010},
	note = {Publisher: W.B. Saunders},
	pages = {141--147}
}

@article{Darden2003,
	title = {Methods of frequency analysis of a complex mammalian vocalisation},
	volume = {13},
	issn = {21650586},
	doi = {10.1080/09524622.2003.9753501},
	abstract = {The prevalence of complex acoustic structures in mammalian vocalisations can make it difficult to quantify frequency characteristics. We describe two methods developed for the frequency analysis of a complex swift fox Vulpes uelox vocalisation, the barking sequence: (1) autocorrelation function analysis and (2) instantaneous frequency analysis. The autocorrelation function analysis results in an energy density spectrum of the signal's averaged amplitude and frequency information. This analysis was used for locating possible formant structures and quantifying the energy distribution of single barks in the barking sequence. The instantaneous frequency analysis is applied to individual continuous frequency bands and generates frequency contours with a resolution of a couple of Hertz. It was used to quantify frequency modulation and calculate average frequencies of harmonic bands in individual barks and to estimate fundamental frequencies. This second method of analysis had to be evaluated with spectrographic analysis to gauge its reliability for each band analysed. The algorithms used should make both of these methods applicable to other complex vocalisations. Keywords:},
	number = {3},
	journal = {Bioacoustics},
	author = {Darden, Safi K. and Pedersen, Simon B. and Dabelsteen, Torben},
	year = {2003},
	pmid = {2833},
	keywords = {Autocorrelation, Instantaneous frequency, Mammalian vocalization, Sound analysis, Vulpes velox},
	pages = {247--263}
}

@article{Yeon2007,
	title = {The vocal communication of canines},
	volume = {2},
	issn = {15587878},
	doi = {10.1016/j.jveb.2007.07.006},
	abstract = {Canine vocalizations can be divided into several types and these types of vocalizations can carry the senders' emotional state to receivers such as other dogs and humans. When humans send a signal consisting of short notes, it can elicit a reactive response and increase motor activity levels more so than a signal consisting of a longer continuous note. This means that humans and dogs can communicate with each other using acoustic signals. These canine vocalizations can be analyzed by computer-aided programs that evaluate several parameters including fundamental frequency, and other frequency variables like minimum and maximum frequency, duration of call, inter-call duration, amplitude variables, harmonic to noise ratio (HNR), nonlinear phenomena, like limit cycle, subharmonic, biphonation, and chaos. Dogs' vocalizations can be analyzed using objective scientific criteria with these parameters, and using this analysis, we know that dog vocalizations fall into several context based vocal types. Also humans can distinguish the difference in dog barks. The dogs' barking can be a behavioral problem because of their high intensity especially in urban areas. Treatment methods may include environmental manipulation, behavior modification, and positive re-enforcement. This paper discusses the literature related to scientific analysis of canine vocalization. © 2007 Elsevier Inc. All rights reserved.},
	number = {4},
	journal = {Journal of Veterinary Behavior: Clinical Applications and Research},
	author = {Yeon, Seong Chan},
	year = {2007},
	note = {ISBN: 1558-7878},
	keywords = {communication, dog, spectrogram, vocalization},
	pages = {141--144}
}

@article{Feddersen-Petersen2000,
	title = {Vocalization of {European} wolves ({Canis} lupus lupus {L}.) and various dog breeds ({Canis} lupus f. fam.)},
	volume = {43},
	issn = {00039438 (ISSN)},
	abstract = {Barking in domestic dogs still remains a topic of controversial discussions. While some authors assess dog- barking an acoustic means of expression becoming more and more sophisticated during domestication, others name this sound type "non-communicative". Vocal repertoires as works on individual sound types are rare, however, and there has been almost no work done on Iow-intensity, close-range vocalizations, yet such types of vocalization are especially important with the more social canids, hence, with the human-dog-communication and understanding of dogs. Most of the investigations published so far are based on auditive sound impressions and lack objectivity. The principal method used in this study was sonagraphic. This facilitates the identiftcation of sounds and reveales, whether subjective Classification can be verified by objectively measured parameters. Finally, meanings, funetions and emotions were examined for all the major sounds described and are discussed in terms of relationships between sound structure and Signal function, signal emission and social context as behavioural response, and overlapping Channels of communication. Ontogeny of acoustic communication in 11 European wolves has been compared to various dog breeds (8 Standard Poodles, 8 Toy Poodles, 15 Kleine Münsterländer, 11 Weimaraner Hunting Dogs, 16 Tervueren, 12 American Staffordshire Terriers, and 13 German Shepherds, 12 Alaskan Malamutes, and 9 Bull Terriers) from birth up to 8 (12) weeks resp. 4 (12) months of age. Noisy and harmonic sound groups were analysed separately as overriding units. Following parameters were used: fmax=maximum of spectrographic pietured sounds (Hz), xfo=mean of the lowest frequency band of harmonic sounds (Hz), xfd=mean of the frequency of strongest amplitude of noisy sounds (Hz), delta f=frequency ränge of sounds (Hz), duration of sounds (ms). Statistical analysis was run on "Statistica", Release 4,0. Within the sound type barking 2 to 12 subunits were classified in the different breeds, aecording to their context-speeifie spectrographic design, and behavioural responses. Categories of function / emotion include f.e. social play, play soliticing, exploration, caregiving, social contact and "greeting", loneliness, and agonistc behaviours. "Interaction" was the most common category of social context for masted barkings (56\% of oecurences). Especially close-range vocalizations, conceming the major sound type of most domestic dogs, the bark, evolved highly variable. However, the ecological niche of domestic dogs is highly variable, just as the individual differences in the dogs are, which seem to be breed-typical to a great extent. Thus, complexity within the dog's vocal repertoire, and therefore enhancement of its communicative value, is achieved by many subunits of bark, some standing for specific motivations, informations and expressions. Complexity within the dogs'vocal repertoire is extended by the use of mixed sounds in the barking context. Transitions and gradations to a great extend oeeur via bark sounds: harmonic, intermediate and noisy subunits.},
	number = {4},
	journal = {Archiv für Tierzucht},
	author = {Feddersen-Petersen, Dorit Urd},
	year = {2000},
	pmid = {930},
	note = {ISBN: 0003-9438},
	keywords = {communication, domestication, functions of barking, funetions of barking, interaction, selection, selection., vocalization},
	pages = {387--397}
}

@article{Farago2010,
	title = {'{The} bone is mine': affective and referential aspects of dog growls},
	volume = {79},
	issn = {00033472},
	doi = {10.1016/j.anbehav.2010.01.005},
	abstract = {A number of species are considered to use functionally referential signals such as alarm calls or food-related vocalizations. However, this particular function of communicative interaction has not previously been found in canids. We provide the first experimental indication that domestic dogs, Canis familiaris, rely on context-dependent signals during interspecific agonistic encounters. We recorded several sequences of growls from dogs in three different contexts: during play, guarding a bone from another dog, and reacting to a threatening stranger. We analysed the acoustic structure of the growls and additionally performed playback tests in a seminatural food-guarding situation. We found that play growls differed acoustically from the other two (agonistic) types of growls, mainly in their fundamental frequencies and formant dispersions. Results of the playback experiment showed that food-guarding growls deterred other dogs from taking away a seemingly unattended bone more effectively than growls recorded in the threatening stranger situation. We ruled out an effect of the signaller's body weight on the subjects' responses. These results provide the first evidence of context specificity of agonistic vocalizations in the dog. We discuss the possible aspects of honesty and deception through acoustic modulation of growls. © 2010 The Association for the Study of Animal Behaviour.},
	number = {4},
	journal = {Animal Behaviour},
	author = {Faragó, Tamás and Pongrácz, Péter and Range, Friederike and Virányi, Zsófia and Miklósi, Ádám},
	year = {2010},
	note = {ISBN: 0003-3472},
	keywords = {Canis familiaris, acoustical analysis, dog, graded signal, growl, referential communication},
	pages = {917--925}
}

@article{Schrader1997,
	title = {Computer-aided analysis of acoustic parameters in animal vocalisations: {A} multi-parametric approach},
	volume = {7},
	issn = {21650586},
	doi = {10.1080/09524622.1997.9753338},
	abstract = {The computer-aided analysis of acoustic signals of mammals is still a problem, as often (a) sound structures are complex, (b) vocal repertoires often comprise an enormous variety of vocalisations, (c) recordings are influenced by the acoustic conditions of the environment, and (d) the distance and spatial orientation of the sender to the microphone changes. In recent software packages for the analysis of acoustic signals, procedures are integrated which allow the calculation of a variety of signal features. However, these algorithms are often problematic under the conditions mentioned above. In this paper, we present a multi-parametric approach which reduces these problems and which allows a quantitative and reproducible analysis of complex animal vocalisations. Our approach comprises the following aspects: (1) reduction of influences of recording conditions, (2) determination of different sound features and (3) calculation of parameters to characterize these sound features. All calculations are done on the basis of the digitized spectrograms. Special attention is given to the use of smoothing algorithms and dynamic thresholds in order to estimate sound features and to reduce influences resulting from recording conditions. The suitability of our approach has been demonstrated successfully for vocalisations of different species.},
	number = {4},
	journal = {Bioacoustics},
	author = {Schrader, Lars and Hammerschmidt, Kurt},
	year = {1997},
	keywords = {Computer sound analysis, Mammals, Multiparametric approach, Primates, Vocalizations},
	pages = {247--265}
}

@article{Riede1999,
	title = {Vocal tract length and acoustics of vocalization in the domestic dog ({Canis} familiaris).},
	volume = {202},
	issn = {0022-0949},
	abstract = {The physical nature of the vocal tract results in the production of formants during vocalisation. In some animals (including humans), receivers can derive information (such as body size) about sender characteristics on the basis of formant characteristics. Domestication and selective breeding have resulted in a high variability in head size and shape in the dog (Canis familiaris), suggesting that there might be large differences in the vocal tract length, which could cause formant behaviour to affect interbreed communication. Lateral radiographs were made of dogs from several breeds ranging in size from a Yorkshire terrier (2.5 kg) to a German shepherd (50 kg) and were used to measure vocal tract length. In addition, we recorded an acoustic signal (growling) from some dogs. Significant correlations were found between vocal tract length, body mass and formant dispersion, suggesting that formant dispersion can deliver information about the body size of the vocalizer. Because of the low correlation between vocal tract length and the first formant, we predict a non-uniform vocal tract shape.},
	journal = {The Journal of experimental biology},
	author = {Riede, T and Fitch, T},
	year = {1999},
	pmid = {10504322},
	note = {ISBN: 0022-0949},
	keywords = {canidae, dog, formant, growling, source-tract theory},
	pages = {2859--2867}
}

@article{Yin2004,
	title = {Barking in domestic dogs: {Context} specificity and individual identification},
	volume = {68},
	issn = {00033472},
	doi = {10.1016/j.anbehav.2003.07.016},
	abstract = {In this study we sought to determine whether dog barks could be divided into subtypes based on context. We recorded barking from 10 adult dogs, Canis familiaris, of six breeds in three different test situations: (1) a disturbance situation in which a stranger rang the doorbell, (2) an isolation situation in which the dog was locked outside or in a room isolated from its owner and (3) a play situation in which either two dogs or a human and a dog played together. We analysed spectrograms of 4672 barks using macros that took 60 sequential frequency measurements and 60 sequential amplitude measurements along the length of the call. Statistical analyses revealed that barks are graded vocalizations that range from harsh, low-frequency, unmodulated calls to harmonically rich, higher-frequency, modulated calls. The harsh, low-frequency, unmodulated barks were more commonly given in the disturbance situation, and the more tonal, higher-pitch, modulated barks were more commonly given in the isolation and play situations. Disturbance barks were also longer in duration with more rapid repetition than the barks given in other contexts. Discriminant analysis revealed that dog barks can be divided into different subtypes based on context even within individual dogs, and that dogs can be identified by their bark spectrograms despite the context of the bark. © 2004 The Association for the Study of Animal Behaviour. Published by Elsevier Ltd. All rights reserved.},
	number = {2},
	journal = {Animal Behaviour},
	author = {Yin, Sophia and McCowan, Brenda},
	year = {2004},
	note = {ISBN: 0003-3472},
	pages = {343--355}
}

@article{Lord2009,
	title = {Barking and mobbing},
	volume = {81},
	issn = {0376-6357},
	url = {https://www.sciencedirect.com/science/article/pii/S0376635709001077},
	doi = {10.1016/J.BEPROC.2009.04.008},
	abstract = {Barking is most often associated with the domestic dog Canis familiaris, but it is a common mammalian and avian vocalization. Like any vocalization, the acoustic character of the bark is likely to be a product of adaptation as well as an expression of the signaler's internal motivational state. While most authors recognize that the bark is a distinct signal type, no consistent description of its acoustic definition or function is apparent. The bark exhibits considerable variability in its acoustic form and occurs in a wide range of behavioral contexts, particularly in dogs. This has led some authors to suggest that dog barking might be a form of referential signaling, or an adaptation for heightened capability to communicate with humans. In this paper we propose a general ‘canonical’ acoustic description of the bark. Surveying relevant literature on dogs, wild canids, other mammals and birds, we explore an alternative functional hypothesis, first suggested by [Morton, E.S., 1977. On the occurrence and significance of motivation-structural rules in some bird and mammal sounds. Am. Nat. 111, 855–869] and consistent with his motivational-structural rules theory: that barking in many animals, including the domestic dog, is associated with mobbing behavior and the motivational states that accompany mobbing.},
	number = {3},
	urldate = {2018-02-05},
	journal = {Behavioural Processes},
	author = {Lord, Kathryn and Feinstein, Mark and Coppinger, Raymond},
	month = jul,
	year = {2009},
	note = {Publisher: Elsevier},
	pages = {358--368}
}