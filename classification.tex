\section{Classification}

In this section, we present the architecture of the classification model in
\sys. 
%
We begin by making the case for a generalized model to enable
performant and accurate content-based audio retrieval
in~\cref{sec:classification-case}.
%
We then present our two-level hierarchy of classifiers that is derived from a
taxonomy of sounds.
%
Lastly, we describe the methods used in \sys to serve as top-level and
bottom-level classifiers in~\cref{sec:classification-methods}.

\subsection{Case for a Generalized Classification Model}
\label{sec:classification-case}

Classification models in the audio domain are typically tailored for a specific
task, such as speech recognition or music genre recognition~\cite{Campbell1997,
tzanetakis-musical-2002}.
%
However, these \textit{specialized} models are not effective for general-purpose
audio recognition (\eg, classification of animal sounds).
%
Given this limitation of prior models, we propose to leverage an ecological
taxonomy of sound to design a novel generalized classification model.

Studies of human auditory event perception have long been of interest to
ecologists.
%
Gaver created a taxonomy of environmental sounds based on physical properties
and human perception~\cite{Gaver1993}.
%
He hypothesized that in general listening tasks, humans rely on complex stimuli 
and not a combination of the senses as was previously thought.
%
If this hypothesis holds, an agent can determine the sources of an audio
recording through audio alone without relying on other senses (\eg, vision).
%
Gaver posits that audio contains sufficient information regarding the items
interacting in the environment and that these interactions can be separated 
into several classes and sub-classes.
%
This hypothesis of hearing breaks the problem of decoding what we hear into  
more generalizable pieces on which an agent can be trained~\cite{Gaver1993}.
%

Gaver only created a taxonomy for interacting materials in the environment.
%
However, other ecologists have subsequently studied human perception of animal
vocalizations and music.
%
In particular, Lewicki examined how animal sounds fit into a generalized
taxonomy of sounds~\cite{lewicki-efficient-2002}.
%
He found that environmental sounds are structurally different from animal
vocalizations in that the latter sounds are longer, harmonic, and have a narrow
bandwidth while the former sounds are transient, non-harmonic, and are
broadband.
%
Lewicki hypothesizes that this dichotomy has intentionally evolved to allow for
easier discrimination from environmental sounds when communicating.
%
Furthermore, he notes that human speech is structurally different from 
normal animal vocalizations in that it contains a mix of harmonic and
non-harmonic structures.
%
Thus, speech and animal vocalizations are two separate branches in the
generalized sound taxonomy.


Ecologists also make a distinction between how we listen to regular sounds  
and how we listen to music~\AJ{Cite?}.
%
As such, even though speech and interacting elements technically make up music,
the manner in which we listen to and classify musical sounds is different. 
%
Thus, we place music in its own branch in the taxonomy since once the agent
determines that an audio recording consists of music, it will begin listening 
to it differently.


\subsection{Two-Level Hierarchy of Classifiers}
\label{sec:classification-hierarchy}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/ensemble-overview.png}
    \caption{An overview of the proposed ensemble classifier.}
    \label{fig:classifier-hierarchy}
\end{figure}

Most classification models are \textit{flat} in that there is no hierarchy
between the classes (\ie, categories) to which an audio recording
belongs~\AJ{Cite?}.
%
We instead design a \textit{hierarchical} classification model based on a
generalized sound taxonomy presented in~\cref{sec:classification-case}.
%
In our hierarchical model, multiple classifiers are trained on the same 
training audio database.
%
Unlike other ensemble methods though, this model trains each classifier with
varying goals~\cite{chou-hierarchical-2003}. 
%
We train a classifier for each level of the two-level tree. 
%
While classifying a recording, the algorithm follows a classification path,
choosing the appropriate child node as each level is evaluated.
%
This approach increases overall performance by breaking the classification 
task into smaller, less complex problems. 
%
Furthermore, such a two-level hierarchical model allows us to quickly filter out
irrelevant recordings, thereby accelerating query processing.

\PP{Error Propagation}
%
Error propagation is a major issue in this approach and if not managed can ruin
it outright. 
%
Each classifier has an associated classification error inherent to it.
%
If the error of higher level classifiers is high, that can seriously impede the
efficacy of the entire model. 
%
We tackle this challenge by leveraging \textit{probabilistic classifiers} that
are asssociated with a measure of certainty.
%
We defer a discussion on how a probabilistic approach attenuates the error
propagation problem in~\cref{sec:retrieval}.

\PP{Imbalanced Classes}
%
Another consideration when working with a hierarchical classifier is whether 
the bottom-level classes have enough examples to train a general model.
%
If the database contains too few examples of a low-level class, the classifiers 
will suffer from overfitting. 
%
To mitigate this issue, we introduce a threshold policy that determines the
minimum number of examples required to allow a classifier to be trained on a
certain class.
%
This results in instances of the model trained on a smaller database being less
precise in their retrieval.
%
We study the sensitivity of our classification model to this threshold policy
in~\cref{sec:experiments-sensitivity}.

\subsection{Classification Methods}
\label{sec:classification-methods}

\begin{table}[t]
    \centering
    \begin{tabular}{ccc}
         & 250   & 2000  \\ \hline
    DNN  & 0.174 & 0.191 \\
    HDNN & 0.246 & 0.130
    \end{tabular}
    \caption{Classification results of a hierarchical DNN and a standard DNN trained on all 50 classes.}
    \label{tab:classifier}
\end{table}

%\begin{itemize}
    % \item Argue non-uniform classifier use
    % \item Filtering Classifiers
    % \item Deciding Classifiers
%\end{itemize}

Specific contexts of audio classification have long been an area of interest and thus certain audio sub-domains have been extensively studied for optimal classification techniques. It is unlikely that in human perception all different listening tasks are accomplished through the same process. Instead there are likely specialized systems that have evolved to best handle specific classification tasks. Using this intuition and the ability to do so through hierarchical classification, we pick and choose classifiers and representations that work best for each sub-domain. As this is a tree hierarchy, we have branch and leaf nodes that are comprised of their own classifier. Here, we term them \textit{Filtering} and \textit{Deciding} classifiers.

\subsubsection{Top-Level Classifiers}

\begin{table}[h]
    \begin{tabular}{c|cc}
        Classifier & Training & Prediction \\ \hline
        SVM & $O(n^2p+n^3)$ & $O(n_{sv}p)$ \\
        KNN & - & $O(np)$ \\
        RFC & $O(n^2\sqrt{p}n_{trees})$ & $O(pn_{trees})$
    \end{tabular}
\caption{Filtering classifiers and their properties.}
\label{tab:filt-class}
\end{table}

Filtering classifiers are the branch nodes of the hierarchy tree. They have this term as their main purpose is to filter the data and throw out unrelated documents such that lower filter and deciding classifiers can execute more quickly. In our experiments, it was found to be desirable for these classifiers to be both lightweight and have uncertainty associated with it. As such, probabilistic classifiers are used in the filtering classifiers. 

Probabilistic classifiers are key to mitigating the issue of error propagation in the hierarchical model. While standard classifiers emulate a function of the form $y=f(x)$ with a single result \textit{y} for each input \textit{y}, a probabilistic classifier instead emulates conditional distributions of the form $P(y|x)$. We are able to use the resulting probabilities given by a classifier to determine its confidence in the prediction made. We use this confidence to allow normally misclassified results to still be considered in later steps. This is done by introducing a threshold $\gamma$ that must be reached to proceed. If the threshold is too low, we lose the performance efficacy of the hierarchical structure. On the other hand, if the threshold is too high then false negatives will ensure some documents with harder to classify instances of a class will never be retrieved.

In this implementation, the following lightweight and probabilistic classifiers were experimented with:
\\
\\
\textbf{Support Vector Machines} are a supervised learning model used for classification that partitions the data by finding the most optimal linear hyper-plane that separates all dimensions of the feature set optimally. Although SVMs inherently separate the data by some linear relationship (decision boundary), non-linear classification can be done by the kernel trick. The kernel trick is the method of mapping the inputs into different feature spaces in which the data is linearly separable. To allow for probabilistic estimation, the SVM is computed with soft-margins. The soft-margins provide a decision boundary with in-built uncertainty through a gradient of certainty that becomes greater as samples are picked further from the boundary. The boundary is calculated by minimizing the the hinge loss function over all samples. This is calculated by

\begin{equation}
    \Big[ \frac{1}{n}\sum^n_{i=1}\max(0,1-y_i(\vec{\omega} \cdot \vec{x_i} - b))\Big] + \lambda ||\vec{\omega}||^2
\end{equation}
\\
where $\omega$ is the normal vector to the hyper-plane, $y_i$ is the target, $\vec{x_i}$ is the features at i, and $\lambda$ determines the margin size and whether $\vec{x_i}$ is on the correct side of the decision boundary. For small values of $\lambda$, the soft-margin SVM will perform similarly to the hard-margin.
\\
\\
\textbf{K-Nearest Neighbors} is a classification method that uses the plurality vote of its neighbor samples by some heuristic of distance to determine its belonging to a class. KNN is known as a \textit{lazy learning} method, meaning the classifier defers its computation until classification evaluation. As such, all points are retained in the model. Any custom heuristic for distance can be used but the most common is Euclidean distance or Manhattan Distance. One of the more challenging parts of this classification approach is the tuning of the neighbor number hyper-parameter as at k=1 the class is determined by the first closest neighbor and at k=n the class is whatever the most numerous of the classes is present. To use this classifier in a probabilistic case, the number of matching neighbors and the distances can be used for determination. Probability is computed such that the radial area surrounding the k neighbors creates a probability distribution. \cite{cheng-evaluating-2009}
\\
\\
\textbf{Probabilistic Random Forests} are a supervised ensemble classification learning method. This method uses a collection of decision tree classifiers and at evaluation time provides the mode of the classification prediction. This method attempts to correct for decision trees tendency to overfit the training set. Specifically, deep trees more often tend to overfit, have low bias, and very high variance. Thus, by training many trees on multiple parts of the data, it is expected that variance will be reduced with only a small increase in bias. To train a random forest, the bagging algorithm is employed. This algorithm takes a training set and selects a random sample with replacement to fit a tree to. The bagging approach decreases the model variance, which as mentioned before is a problem in decision trees, and only slightly increases the bias. Evaluation of the model is straightforward as it is either an average of the data in the regression case or is majority vote in classification. To provide probability to the random forest classifier, the choice between two paths is treated as a probability density function (pdf) and both branches are evaluated if it meets some criterion for acceptable probability bounds \cite{reis-probabilistic-2018}. This approach modifies the \textit{Gini impurity} function

\begin{equation}
    G_n \xrightarrow{}    \bar{G_n} = 1 - \sum_{j=1 \dotsc k} \bar{P}_{n,A_j}
\end{equation}

% \subsubsection{Annotation Normalization} Annotation normalization is carried
% out on subsets of both the ESC-50 and AudioSet database. Performance is
% measured in terms of execution time, efficiency, and overall annotation
% reduction. The normalization performance is measured against human performance
% and Hsu's normalization results \cite{Hsu2008}. As ESC-50 already has its
% annotations manually normalized, the normalization experiments demonstrate if
% the techniques go too far and overgeneralize. Additionally, it provides a
% clean test-bed for hypernyms to be evaluated. For hypernym sets, we hope to
% have sets that as much as possible attempt to distribute the database across
% them. AudioSet presents a much greater challenge for the low level
% normalization task. These experiments give insight into how the technique
% works at scale and what bottlenecks exist.

\subsubsection{Bottom-Level Classifiers}

\begin{table}[h]
    \begin{tabular}{c|cc}
        Classifier & Training & Prediction \\ \hline
        DNN & $O(n^4)$ & $O(n^5)$ \\
        RNN & $O(n^4)$ & $O(n^5)$ \\
        GMM & $NP$ & $a$
    \end{tabular}
    \caption{Decision classifiers and their properties.}
    \label{tab:dec-class}
\end{table}
Deciding classifiers are those found on the leaf nodes of the hierarchy tree. These classifiers have the purpose of making some determination on the remaining data and as such are able to be much more definitive with their result. In particular, this means the models at these leaf nodes are able to be more complex and take longer per sample to execute as ideally much of the database has been ruled out already. We do however, retain probabilistic classification because it serves as a heuristic for ranking the retrieved documents.
\\
\\
\textbf{Deep Neural Networks} are a method of supervised classification that forms a feed-forward graph of connected neurons that only activate if some threshold is met. A neural network is meant to model a collection of neurons in the human brain with them only activating if sufficiently excited. Even though this model falls short of actually modeling neurons, it performs well in many applications. Network neurons are organized into layers that form a row of neurons that is normally fully connected to the previous row or, in the base case, the inputs. A network can be as simple as a single layer, but in most applications a large number of layers and neurons are required. Unfortunately, the algorithm that trains these networks is computationally expensive and on single thread computers can take hours to converge. However, the algorithm is embarrassingly parallel and thus can be much more efficiently performed on GPU or TPU hardware. This allows for networks to be much larger and more complex without needing to sacrifice training time, these networks are known as Deep Neural Networks (DNN). To determine the probability of each class predicted, the final layer is evaluated with the softmax function as in probability theory it is used to represent a categorical distribution.
\\
\\
\textbf{Recurrent Neural Networks} are an extension of neural network with the major difference that they have a temporal component. To account for this new component, the backpropagation algorithm is modified to the backpropagation through time algorithm in which time is expresed by an ordered series of calculations linking one time step to the next allowing backpropagation to proceed as usual. The process of carrying memory forward can be described as

\begin{equation}
    h_t=\phi(Wx_t+Uh_{t-1})
\end{equation}
\\
with $h_t$ the hidden state at time t, $x_t$ the input at time t, $W$ the weight matrix, and $U$ the transition matrix. Although there is more information from the past, these neural networks also share the vanishing gradient issue deep neural networks have. Long Short-Term Memory units (LSTMs) were introduced in an attempt to solve the vanishing gradient problem. LSTMs contain information in a gated cell that can be written to or read from and only allows these operations if some analog value passes a threshold. By gating with a differential value, the backpropagation algorithm can iteratively determine when it is best to recall, write, or forget the stored information. This allows the network to use data from layers more than one layer behind it.
\\
\\
\textbf{Gaussian Mixture Model} is a probabilistic model assuming the data are generated by a number of Gaussian distributions with unknown parameters. GMMs are able to be trained either in an unsupervised or supervised manner, in this approach we use the supervised approach. The model is trained using the expectation-maximization algorithm which determines the component weights, means, and variances. Although this approach has seen success on many audio applications in the past, training complexity is high and thus training on large datasets is time intensive. This model is naturally probabilistic and as such no extra steps need to be taken for probability estimation.
\\
